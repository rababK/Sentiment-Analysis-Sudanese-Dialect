{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8a5e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 05:17:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2aa60249d74814be9e33c0113a1d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 05:17:32 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2022-09-27 05:17:32 INFO: Use device: cpu\n",
      "2022-09-27 05:17:32 INFO: Loading: tokenize\n",
      "2022-09-27 05:17:34 INFO: Loading: mwt\n",
      "2022-09-27 05:17:34 INFO: Loading: pos\n",
      "2022-09-27 05:17:42 INFO: Loading: lemma\n",
      "2022-09-27 05:17:42 INFO: Loading: depparse\n",
      "2022-09-27 05:17:45 INFO: Loading: ner\n",
      "2022-09-27 05:18:07 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"الرئيس اوباما\",\n",
      "  \"type\": \"PER\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 18\n",
      "}, {\n",
      "  \"text\": \"هاواي\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 22,\n",
      "  \"end_char\": 27\n",
      "}]\n",
      "---------------------------------------------------\n",
      "[({\n",
      "  \"id\": 2,\n",
      "  \"text\": \"ولد\",\n",
      "  \"lemma\": \"وَلَد\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"N------S1R\",\n",
      "  \"feats\": \"Case=Nom|Definite=Cons|Number=Sing\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 1,\n",
      "  \"end_char\": 4\n",
      "}, 'punct', {\n",
      "  \"id\": 1,\n",
      "  \"text\": \".\",\n",
      "  \"lemma\": \".\",\n",
      "  \"upos\": \"PUNCT\",\n",
      "  \"xpos\": \"G---------\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"punct\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 1\n",
      "}), ({\n",
      "  \"id\": 0,\n",
      "  \"text\": \"ROOT\"\n",
      "}, 'root', {\n",
      "  \"id\": 2,\n",
      "  \"text\": \"ولد\",\n",
      "  \"lemma\": \"وَلَد\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"N------S1R\",\n",
      "  \"feats\": \"Case=Nom|Definite=Cons|Number=Sing\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 1,\n",
      "  \"end_char\": 4\n",
      "}), ({\n",
      "  \"id\": 2,\n",
      "  \"text\": \"ولد\",\n",
      "  \"lemma\": \"وَلَد\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"N------S1R\",\n",
      "  \"feats\": \"Case=Nom|Definite=Cons|Number=Sing\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 1,\n",
      "  \"end_char\": 4\n",
      "}, 'nmod', {\n",
      "  \"id\": 3,\n",
      "  \"text\": \"الرئيس\",\n",
      "  \"lemma\": \"رَئِيس\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"N------S2D\",\n",
      "  \"feats\": \"Case=Gen|Definite=Def|Number=Sing\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"nmod\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 11\n",
      "}), ({\n",
      "  \"id\": 3,\n",
      "  \"text\": \"الرئيس\",\n",
      "  \"lemma\": \"رَئِيس\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"N------S2D\",\n",
      "  \"feats\": \"Case=Gen|Definite=Def|Number=Sing\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"nmod\",\n",
      "  \"start_char\": 5,\n",
      "  \"end_char\": 11\n",
      "}, 'nmod', {\n",
      "  \"id\": 4,\n",
      "  \"text\": \"اوباما\",\n",
      "  \"lemma\": \"اوباما\",\n",
      "  \"upos\": \"X\",\n",
      "  \"xpos\": \"X---------\",\n",
      "  \"feats\": \"Foreign=Yes\",\n",
      "  \"head\": 3,\n",
      "  \"deprel\": \"nmod\",\n",
      "  \"start_char\": 12,\n",
      "  \"end_char\": 18\n",
      "}), ({\n",
      "  \"id\": 6,\n",
      "  \"text\": \"هاواي\",\n",
      "  \"lemma\": \"هاواي\",\n",
      "  \"upos\": \"X\",\n",
      "  \"xpos\": \"U---------\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"nmod\",\n",
      "  \"start_char\": 22,\n",
      "  \"end_char\": 27\n",
      "}, 'case', {\n",
      "  \"id\": 5,\n",
      "  \"text\": \"في\",\n",
      "  \"lemma\": \"فِي\",\n",
      "  \"upos\": \"ADP\",\n",
      "  \"xpos\": \"P---------\",\n",
      "  \"feats\": \"AdpType=Prep\",\n",
      "  \"head\": 6,\n",
      "  \"deprel\": \"case\",\n",
      "  \"start_char\": 19,\n",
      "  \"end_char\": 21\n",
      "}), ({\n",
      "  \"id\": 2,\n",
      "  \"text\": \"ولد\",\n",
      "  \"lemma\": \"وَلَد\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"N------S1R\",\n",
      "  \"feats\": \"Case=Nom|Definite=Cons|Number=Sing\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 1,\n",
      "  \"end_char\": 4\n",
      "}, 'nmod', {\n",
      "  \"id\": 6,\n",
      "  \"text\": \"هاواي\",\n",
      "  \"lemma\": \"هاواي\",\n",
      "  \"upos\": \"X\",\n",
      "  \"xpos\": \"U---------\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"nmod\",\n",
      "  \"start_char\": 22,\n",
      "  \"end_char\": 27\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "doc = nlp2('.ولد الرئيس اوباما في هاواي')\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.ents)\n",
    "print(\"---------------------------------------------------\")\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9984a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 05:19:31 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f051e8457874d24957fc78a250a83a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 05:19:49 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2022-09-27 05:19:49 INFO: Use device: cpu\n",
      "2022-09-27 05:19:49 INFO: Loading: tokenize\n",
      "2022-09-27 05:19:50 INFO: Loading: mwt\n",
      "2022-09-27 05:19:53 INFO: Loading: pos\n",
      "2022-09-27 05:19:58 INFO: Loading: lemma\n",
      "2022-09-27 05:19:58 INFO: Loading: depparse\n",
      "2022-09-27 05:20:01 INFO: Loading: ner\n",
      "2022-09-27 05:25:52 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: الرئيس اوباما\ttype: PER\n",
      "entity: هاواي\ttype: LOC\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "doc = nlp2('.ولد الرئيس اوباما في هاواي')\n",
    "for sent in ([f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents]):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3f71309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 06:40:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9e4f030e644c1d926d387550d4d643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 06:41:20 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2022-09-27 06:41:20 INFO: Use device: cpu\n",
      "2022-09-27 06:41:20 INFO: Loading: tokenize\n",
      "2022-09-27 06:41:27 INFO: Loading: mwt\n",
      "2022-09-27 06:41:28 INFO: Loading: pos\n",
      "2022-09-27 06:41:37 INFO: Loading: lemma\n",
      "2022-09-27 06:41:38 INFO: Loading: depparse\n",
      "2022-09-27 06:41:43 INFO: Loading: ner\n",
      "2022-09-27 06:41:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "\n",
    "\n",
    "    \n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"commentTextExtraction\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            doc = nlp2(row)\n",
    "            for sent in ([f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents]):\n",
    "                file2.write(sent)\n",
    "                file2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef0652fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 07:23:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb42f84fc9fb4dd2bac6e57c243cd2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 07:24:35 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2022-09-27 07:24:35 INFO: Use device: cpu\n",
      "2022-09-27 07:24:35 INFO: Loading: tokenize\n",
      "2022-09-27 07:24:45 INFO: Loading: mwt\n",
      "2022-09-27 07:24:47 INFO: Loading: pos\n",
      "2022-09-27 07:24:57 INFO: Loading: lemma\n",
      "2022-09-27 07:24:58 INFO: Loading: depparse\n",
      "2022-09-27 07:25:04 INFO: Loading: ner\n",
      "2022-09-27 07:25:35 INFO: Done loading processors!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m file1:\n\u001b[0;32m      9\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp2(row)\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m ([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlemma: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep\u001b[38;5;241m.\u001b[39mlemma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m sent\u001b[38;5;241m.\u001b[39mdependencies]):\n\u001b[0;32m     11\u001b[0m         file2\u001b[38;5;241m.\u001b[39mwrite(sent)\n\u001b[0;32m     12\u001b[0m         file2\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m file1:\n\u001b[0;32m      9\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp2(row)\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m ([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlemma: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep\u001b[38;5;241m.\u001b[39mlemma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m sent\u001b[38;5;241m.\u001b[39mdependencies]):\n\u001b[0;32m     11\u001b[0m         file2\u001b[38;5;241m.\u001b[39mwrite(sent)\n\u001b[0;32m     12\u001b[0m         file2\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "\n",
    "\n",
    "    \n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"commentsLemmatizaion\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            doc = nlp2(row)\n",
    "            for sent in ([f'entity: {dep.text}\\lemma: {dep.lemma}' for sent in doc.sentences for dep in sent.dependencies]):\n",
    "                file2.write(sent)\n",
    "                file2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd74a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 07:29:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9fe0bc8fd74ca2a7ac791408052e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 07:30:50 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2022-09-27 07:30:50 INFO: Use device: cpu\n",
      "2022-09-27 07:30:50 INFO: Loading: tokenize\n",
      "2022-09-27 07:30:56 INFO: Loading: mwt\n",
      "2022-09-27 07:30:57 INFO: Loading: pos\n",
      "2022-09-27 07:31:13 INFO: Loading: lemma\n",
      "2022-09-27 07:31:14 INFO: Loading: depparse\n",
      "2022-09-27 07:31:17 INFO: Loading: ner\n",
      "2022-09-27 07:31:43 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "[({\n",
      "  \"id\": 0,\n",
      "  \"text\": \"ROOT\"\n",
      "}, 'root', {\n",
      "  \"id\": 1,\n",
      "  \"text\": \"ولد\",\n",
      "  \"lemma\": \"وَلَد\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VP-P-3MS--\",\n",
      "  \"feats\": \"Aspect=Perf|Gender=Masc|Number=Sing|Person=3|Voice=Pass\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 3\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "doc = nlp2('ولد')\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de7740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 07:50:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663eb763674b4632b2a668d53ca86d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 07:51:02 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2022-09-27 07:51:02 INFO: Use device: cpu\n",
      "2022-09-27 07:51:02 INFO: Loading: tokenize\n",
      "2022-09-27 07:51:04 INFO: Loading: mwt\n",
      "2022-09-27 07:51:04 INFO: Loading: pos\n",
      "2022-09-27 07:51:06 INFO: Loading: lemma\n",
      "2022-09-27 07:51:06 INFO: Loading: depparse\n",
      "2022-09-27 07:51:09 INFO: Loading: ner\n",
      "2022-09-27 07:51:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "وَلَد\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "doc = nlp2('ولد')\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.dependencies[0][2].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8b2c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 08:08:00 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d26c9a34d754525854d8b93a9658902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 08:08:17 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2022-09-27 08:08:17 INFO: Use device: cpu\n",
      "2022-09-27 08:08:17 INFO: Loading: tokenize\n",
      "2022-09-27 08:08:22 INFO: Loading: mwt\n",
      "2022-09-27 08:08:23 INFO: Loading: pos\n",
      "2022-09-27 08:08:37 INFO: Loading: lemma\n",
      "2022-09-27 08:08:56 INFO: Loading: depparse\n",
      "2022-09-27 08:08:59 INFO: Loading: ner\n",
      "2022-09-27 08:09:09 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "\n",
    "\n",
    "    \n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"commentsLemmatizaion\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            doc = nlp2(row)\n",
    "            for sentence in doc.sentences:\n",
    "                file2.write(f'entity: {sentence.dependencies[0][2].text}\\lemma: {sentence.dependencies[0][2].lemma}')\n",
    "                file2.write('\\n')\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c571afa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: invalid syntax (2207338436.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    file2.write(f'entity: {sentence.dependencies[][2].text}\\lemma: {sentence.dependencies[0][2].lemma}')\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "\n",
    "\n",
    "    \n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"commentsLemmatizaion2\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            doc = nlp2(row)\n",
    "            for sentence in doc.sentences:\n",
    "                file2.write(f'entity: {sentence.dependencies[][2].text}\\lemma: {sentence.dependencies[0][2].lemma}')\n",
    "                file2.write('\\n')\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd31d506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 11:59:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42164c47cd5b4e488367b9e5ef0f8c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp2 \u001b[38;5;241m=\u001b[39m \u001b[43mstanza\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommentsWithNoStopWords\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file1:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommentsLemmatizaion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file2:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanza\\pipeline\\core.py:238\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, proxies, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m     download_list \u001b[38;5;241m=\u001b[39m flatten_processor_list(download_list)\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# download_models will skip models we already have\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[43mdownload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mresources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mresources_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlog_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_kwargs(kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_list)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanza\\resources\\common.py:508\u001b[0m, in \u001b[0;36mdownload_models\u001b[1;34m(download_list, resources, lang, model_dir, resources_version, model_url, proxies, log_info)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m download_list:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m         \u001b[43mrequest_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresources_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmd5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43malternate_md5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malternate_md5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    518\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot find the following processor and model name combination: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    519\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please check if you have provided the correct model name.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    520\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanza\\resources\\common.py:139\u001b[0m, in \u001b[0;36mrequest_file\u001b[1;34m(url, path, proxies, md5, raise_for_status, log_info, alternate_md5)\u001b[0m\n\u001b[0;32m    137\u001b[0m basedir \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    138\u001b[0m ensure_dir(basedir)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfile_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_info:\n\u001b[0;32m    141\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanza\\resources\\common.py:96\u001b[0m, in \u001b[0;36mfile_exists\u001b[1;34m(path, md5)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfile_exists\u001b[39m(path, md5):\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    Check if the file at `path` exists and match the provided md5 value.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mget_md5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m md5\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanza\\resources\\common.py:71\u001b[0m, in \u001b[0;36mget_md5\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_md5\u001b[39m(path):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    Get the MD5 value of a path.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m     72\u001b[0m         data \u001b[38;5;241m=\u001b[39m fin\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hashlib\u001b[38;5;241m.\u001b[39mmd5(data)\u001b[38;5;241m.\u001b[39mhexdigest()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "\n",
    "\n",
    "    \n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"commentsLemmatizaion\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            doc = nlp2(row)\n",
    "            for sentence in doc.sentences:\n",
    "                file2.write(f'entity: {sentence.dependencies[0][2].text}\\lemma: {sentence.dependencies[0][2].lemma}')\n",
    "                file2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1382a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 12:16:27 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cc78a59b16486aa8b1e0314d2f44d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 12:16:53 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2022-09-27 12:16:54 INFO: Use device: cpu\n",
      "2022-09-27 12:16:54 INFO: Loading: tokenize\n",
      "2022-09-27 12:17:00 INFO: Loading: mwt\n",
      "2022-09-27 12:17:01 INFO: Loading: pos\n",
      "2022-09-27 12:17:10 INFO: Loading: lemma\n",
      "2022-09-27 12:17:11 INFO: Loading: depparse\n",
      "2022-09-27 12:17:15 INFO: Loading: ner\n",
      "2022-09-27 12:17:57 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "\n",
    "\n",
    "    \n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"commentsLemmatizaion2\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            doc = nlp2(row)\n",
    "            for sent in doc.sentences:\n",
    "                for word in sent.words:\n",
    "                    file2.write(f'word: {word.text+\" \"}\\tlemma: {word.lemma}')\n",
    "                    file2.write('\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f2fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
