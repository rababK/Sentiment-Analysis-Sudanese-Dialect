{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d77c91",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 93: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m                 file2\u001b[38;5;241m.\u001b[39mwrite(commentWithNoStopWords)\n\u001b[0;32m     31\u001b[0m                 file2\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mnormlizeComment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCommentsNEW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormlizedComments2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m removeStopWords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormlizedComments2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommentsWithNoStopWords2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m read_file \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommentsWithNoStopWords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mnormlizeComment\u001b[1;34m(file1, file2)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file1:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file2:\n\u001b[1;32m---> 12\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m file1:\n\u001b[0;32m     15\u001b[0m             file2\u001b[38;5;241m.\u001b[39mwrite(normalize_arabic_text(row))\n\u001b[0;32m     16\u001b[0m             file2\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 93: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "from aranorm import normalize_arabic_text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def normlizeComment(file1,file2):\n",
    "\n",
    "    with open(file1, 'r') as file1:\n",
    "        with open(file2, 'w',newline='') as file2:\n",
    "            for row in file1:\n",
    "       \n",
    "       \n",
    "                file2.write(normalize_arabic_text(row))\n",
    "                file2.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "def removeStopWords(file1,file2):\n",
    "\n",
    "    arabicStopWords= stopwords.words(\"arabic\")\n",
    "\n",
    "    with open(file1, 'r',encoding=\"utf8\") as file1:\n",
    "        with open(file2, 'w',newline='',encoding=\"utf8\") as file2:\n",
    "\n",
    "            for row in file1:\n",
    "                tokenizedRow = word_tokenize(row)\n",
    "                commentWithNoStopWords= ' '.join([i for i in tokenizedRow if i not in arabicStopWords])\n",
    "                file2.write(commentWithNoStopWords)\n",
    "                file2.write('\\n')\n",
    "\n",
    "normlizeComment('CommentsNEW','normlizedComments2')\n",
    "removeStopWords('normlizedComments2','commentsWithNoStopWords2')\n",
    "       \n",
    "read_file = pd.read_csv (r'commentsWithNoStopWords')\n",
    "read_file.to_csv (r'cleanComments.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 16:12:53 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ac2b2650674787bac2d245715133d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096842d95603460b8c208920f0151daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ar/resolve/v1.4.1/models/depparse/padt.pt:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "def remove_duplicated_characters(text):\n",
    "    result=[]\n",
    "    text=''.join(i for i,_ in itertools.groupby(text))\n",
    "    result.append(text)\n",
    "    return ''.join(result)\n",
    "    \n",
    "\n",
    "def remove_duplicated_words(text):\n",
    "    s1=[]\n",
    "    text=' '.join(k for k,v in itertools.groupby(text.replace(\"&lt;/Sent&gt;\",\"\").split()))\n",
    "    s=re.sub(r'b(.+)(\\s+\\b)+',r'\\1',text)\n",
    "    return ''.join(s)\n",
    "    \n",
    "    \n",
    "with open(\"commentsWithNoStopWords2\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"commentsWithNoDuplicated2\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            phace1=remove_duplicated_characters(row)\n",
    "            phace2=remove_duplicated_words(phace1)\n",
    "            file2.write(phace2)\n",
    "            file2.write('\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec912e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "def remove_duplicated_characters(text):\n",
    "    result=[]\n",
    "    text=''.join(i for i,_ in itertools.groupby(text))\n",
    "    result.append(text)\n",
    "    return ''.join(result)\n",
    "    \n",
    "\n",
    "def remove_duplicated_words(text):\n",
    "    s1=[]\n",
    "    text=' '.join(k for k,v in itertools.groupby(text.replace(\"&lt;/Sent&gt;\",\"\").split()))\n",
    "    s=re.sub(r'b(.+)(\\s+\\b)+',r'\\1',text)\n",
    "    return ''.join(s)\n",
    "    \n",
    "    \n",
    "with open(\"commentsWithNoStopWords2\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"commentsWithNoDuplicated2\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            phace1=remove_duplicated_characters(row)\n",
    "            phace2=remove_duplicated_words(phace1)\n",
    "            file2.write(phace2)\n",
    "            file2.write('\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce5cd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d304824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "def remove_duplicated_words(text):\n",
    "    s1=[]\n",
    "    text=' '.join(k for k,v in itertools.groupby(text.replace(\"&lt;/Sent&gt;\",\"\").split()))\n",
    "    s=re.sub(r'b(.+)(\\s+\\b)+',r'\\1',text)\n",
    "    return ''.join(s)\n",
    "    \n",
    "    \n",
    "with open(\"commentsWithNoDuplicated2\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"commentsWithNoDuplicatedword\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            phace2=remove_duplicated_words(row)\n",
    "            file2.write(phace2)\n",
    "            file2.write('\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04732fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 17:48:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bc7ce0d34f4ee6b93f66c9e8ee0a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4663bbad5de841bbbfe088ff37f20b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ar/resolve/v1.4.1/models/depparse/padt.pt:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "md5 for C:\\Users\\Athar\\stanza_resources\\ar\\depparse\\padt.pt is 37758183df3926d82a2b70c23aabf745, expected 4179d9655d4d22f0d5e1e9c817889269",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp2 \u001b[38;5;241m=\u001b[39m \u001b[43mstanza\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommentsWithNoDuplicatedword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file1:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllcommentsLemmatizaionV2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file2:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanza\\pipeline\\core.py:238\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, proxies, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m     download_list \u001b[38;5;241m=\u001b[39m flatten_processor_list(download_list)\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# download_models will skip models we already have\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[43mdownload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mresources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mresources_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlog_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_kwargs(kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_list)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanza\\resources\\common.py:508\u001b[0m, in \u001b[0;36mdownload_models\u001b[1;34m(download_list, resources, lang, model_dir, resources_version, model_url, proxies, log_info)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m download_list:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m         \u001b[43mrequest_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresources_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmd5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43malternate_md5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malternate_md5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    518\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot find the following processor and model name combination: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    519\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please check if you have provided the correct model name.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    520\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanza\\resources\\common.py:154\u001b[0m, in \u001b[0;36mrequest_file\u001b[1;34m(url, path, proxies, md5, raise_for_status, log_info, alternate_md5)\u001b[0m\n\u001b[0;32m    152\u001b[0m     download_file(url, temppath, proxies, raise_for_status)\n\u001b[0;32m    153\u001b[0m     os\u001b[38;5;241m.\u001b[39mreplace(temppath, path)\n\u001b[1;32m--> 154\u001b[0m \u001b[43massert_file_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malternate_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanza\\resources\\common.py:107\u001b[0m, in \u001b[0;36massert_file_exists\u001b[1;34m(path, md5, alternate_md5)\u001b[0m\n\u001b[0;32m    105\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound a possibly older version of file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, md5 \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, path, alternate_md5, md5)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmd5 for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, expected \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (path, file_md5, md5))\n",
      "\u001b[1;31mValueError\u001b[0m: md5 for C:\\Users\\Athar\\stanza_resources\\ar\\depparse\\padt.pt is 37758183df3926d82a2b70c23aabf745, expected 4179d9655d4d22f0d5e1e9c817889269"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp2 = stanza.Pipeline('ar')\n",
    "\n",
    "\n",
    "    \n",
    "with open(\"commentsWithNoDuplicatedword\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"AllcommentsLemmatizaionV2\", 'w',newline='',encoding=\"utf8\") as file2:\n",
    "        for row in file1:\n",
    "            doc = nlp2(row)\n",
    "            for sent in doc.sentences:\n",
    "                for word in sent.words:\n",
    "                    file2.write(word.lemma+\" \")\n",
    "                file2.write('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8dc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
