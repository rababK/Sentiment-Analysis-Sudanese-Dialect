{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9927792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['اجمل' 'الاشتراك' 'الباقه' 'الرساءل' 'تشرحو' 'دي' 'رمز' 'شنو' 'طريقه'\n",
      " 'طيب' 'قللتو' 'ليها' 'ميقات' 'واديتونا' 'يكون']\n",
      "(3, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "  \n",
    "        \n",
    "     'طيب تشرحو ليها رمز دي طريقه الاشتراك الباقه دي.',\n",
    "     '.قللتو الرساءل دي واديتونا ليها ميقات يكون اجمل',\n",
    "     '.رمز الاشتراك شنو',\n",
    " ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a934e3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'an' 'best' 'game' 'great' 'is' 'of' 'series' 'so' 'the'\n",
      " 'thrones' 'tv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "\n",
    "wordset =[\n",
    "   'Game of Thrones is an amazing tv series!',\n",
    "   'Game of Thrones is the best tv series!',\n",
    "   'Game of Thrones is so great',\n",
    "]\n",
    "\n",
    "\n",
    "l_doc1 = re.sub(r\"[^a-zA-Z0-9]\", \" \", doc1.lower()).split()\n",
    "l_doc2 = re.sub(r\"[^a-zA-Z0-9]\", \" \", doc2.lower()).split()\n",
    "l_doc3 = re.sub(r\"[^a-zA-Z0-9]\", \" \", doc3.lower()).split()\n",
    "\n",
    "\n",
    "def calculateBOW(wordset,l_doc):\n",
    "  tf_diz = dict.fromkeys(' ',\"sen\")\n",
    "\n",
    "  for word in l_doc:\n",
    "      tf_diz[word]=l_doc.count(word)\n",
    "\n",
    "  return tf_diz\n",
    "\n",
    "bow1 = calculateBOW(wordset,l_doc1)\n",
    "bow2 = calculateBOW(wordset,l_doc2)\n",
    "bow3 = calculateBOW(wordset,l_doc3)\n",
    "df_bow = pd.DataFrame([bow1,bow2,bow3])\n",
    "df_bow.head()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6346a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 270)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "wordset = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    c=0\n",
    "    for row in file1:\n",
    "        if c<50:\n",
    "            wordset.append(row)\n",
    "            c=c+1\n",
    "       \n",
    "vectorizer =  CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "\n",
    "df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow_sklearn.head()\n",
    "\n",
    "df_bow_sklearn.to_csv('BOW2.csv', encoding='utf-8')\n",
    "\n",
    "file2.close()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ddd0e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 270)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "wordset = [\n",
    "\n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    for row in file1:\n",
    "            wordset.append(row)\n",
    "        \n",
    "       \n",
    "vectorizer =  CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "\n",
    "df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow_sklearn.head()\n",
    "\n",
    "df_bow_sklearn.to_csv('BOW3.csv', encoding='utf-8')\n",
    "\n",
    "file2.close()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761d2256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1921)\t0.4792561516861785\n",
      "  (0, 756)\t0.43288847457257656\n",
      "  (0, 353)\t0.313029767988369\n",
      "  (0, 995)\t0.4521327993291748\n",
      "  (0, 2562)\t0.3051507403879635\n",
      "  (0, 2156)\t0.43288847457257656\n",
      "  (1, 1234)\t0.23923718083170847\n",
      "  (1, 219)\t0.4103235280645333\n",
      "  (1, 189)\t0.3877088831278344\n",
      "  (1, 1521)\t0.4437003239871909\n",
      "  (1, 937)\t0.5087697125207767\n",
      "  (1, 1531)\t0.4103235280645333\n",
      "  (2, 30)\t0.36106094840913283\n",
      "  (2, 2548)\t0.3298372507973552\n",
      "  (2, 2093)\t0.39973501449669907\n",
      "  (2, 1876)\t0.32238688232156665\n",
      "  (2, 2207)\t0.39973501449669907\n",
      "  (2, 330)\t0.37711213608506106\n",
      "  (2, 1719)\t0.39973501449669907\n",
      "  (2, 1234)\t0.1879661379096089\n",
      "  (3, 1457)\t0.45111455557071134\n",
      "  (3, 1296)\t0.6942606978229787\n",
      "  (3, 189)\t0.5608009818202802\n",
      "  (5, 189)\t1.0\n",
      "  (6, 1871)\t0.38998829851435735\n",
      "  :\t:\n",
      "  (942, 1952)\t0.7697980646403396\n",
      "  (942, 1157)\t0.5772768001053054\n",
      "  (942, 1394)\t0.27232780933310335\n",
      "  (943, 1246)\t0.7697980646403396\n",
      "  (943, 1157)\t0.5772768001053054\n",
      "  (943, 1394)\t0.27232780933310335\n",
      "  (944, 407)\t0.7282187490288227\n",
      "  (944, 308)\t0.6350828025448557\n",
      "  (944, 1394)\t0.25761849210541654\n",
      "  (945, 198)\t0.8294867818462057\n",
      "  (945, 365)\t0.4638979446287559\n",
      "  (945, 1394)\t0.31104722424680353\n",
      "  (946, 437)\t0.6859701534812822\n",
      "  (946, 510)\t0.6859701534812822\n",
      "  (946, 1394)\t0.24267240688988992\n",
      "  (947, 1417)\t0.9363330070353189\n",
      "  (947, 1394)\t0.3511132295089398\n",
      "  (948, 1539)\t0.46542654467800737\n",
      "  (948, 971)\t0.46542654467800737\n",
      "  (948, 904)\t0.46542654467800737\n",
      "  (948, 614)\t0.43908587461422366\n",
      "  (948, 1081)\t0.3608708820776578\n",
      "  (948, 1394)\t0.16465174068326116\n",
      "  (949, 653)\t0.7347435161440878\n",
      "  (949, 1872)\t0.6783450195027767\n",
      "['ءاجترار' 'االلهم' 'االنت' ... 'يومي' 'يومين' 'يوميين']\n",
      "(950, 2566)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "wordset = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "\n",
    "    for row in file1:\n",
    "        wordset.append(row)\n",
    "       \n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "print(X)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "with open(\"commentsWithTFIDF\", 'w',encoding=\"utf8\") as file2:\n",
    "    file2.write(str(result))\n",
    "file2.close()\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9c6144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(file1)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    \n",
    "    with open(\"TFIDFDENSE\", 'w',encoding=\"utf8\") as file2:\n",
    "        for i in range(len(denselist)):\n",
    "            \n",
    "            file2.write(str(i)+\" - \"+ str(denselist[0][i])+ \"\\t\"+ str(denselist[1][i]))\n",
    "            file2.write(\"\\n\")\n",
    "    file2.close()\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92d5d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(file1)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    \n",
    "    with open(\"feature_names\", 'w',encoding=\"utf8\") as file2:\n",
    "        for i in feature_names:\n",
    "            file2.write(str(i+\" \\n\"))\n",
    "    file2.close()\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "627cf20b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file2:\n\u001b[0;32m      5\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 6\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfile1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      8\u001b[0m     dense \u001b[38;5;241m=\u001b[39m vectors\u001b[38;5;241m.\u001b[39mtodense()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[38;5;124;03m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[0;32m   2059\u001b[0m \n\u001b[0;32m   2060\u001b[0m \u001b[38;5;124;03mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m-> 2077\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"feature_names\", 'r',encoding=\"utf8\") as file2:\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectors = vectorizer.fit_transform([file1, file2])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        \n",
    "        \n",
    "        with open(\"TFIDFwithfeture\", 'w',encoding=\"utf8\") as file3:\n",
    "            for i in range(len(denselist)):\n",
    "\n",
    "                file3.write(str(i)+\" - \"+ str(denselist[0][i])+ \"\\t\"+ str(denselist[1][i]))\n",
    "                file3.write(\"\\n\")\n",
    "        file3.close()\n",
    "    \n",
    "\n",
    "\n",
    "        df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35255f47",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommentsWithNoStopWords\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file1:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTFIDFwithfeture\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file2:\n\u001b[1;32m---> 17\u001b[0m         \u001b[43mcomputeIDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfile1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     file2\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     19\u001b[0m file1\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36mcomputeIDF\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      3\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(documents)\n\u001b[1;32m----> 5\u001b[0m idfDict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\u001b[43mdocuments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m(), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word, val \u001b[38;5;129;01min\u001b[39;00m document\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"TFIDFwithfeture\", 'r',encoding=\"utf8\") as file2:\n",
    "        computeIDF([file1,file2])\n",
    "    file2.close()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c27afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(950, 2566)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "wordset = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "\n",
    "    for row in file1:\n",
    "\n",
    "            wordset.append(row)\n",
    "\n",
    "       \n",
    "vectorizer =  CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "features=vectorizer.get_feature_names_out()\n",
    "with open(\"featureWithBOW.csv\", 'w',encoding=\"utf8\") as file2:\n",
    "    for i in features:\n",
    "        file2.write(str(i+\" \\n\"))\n",
    "        \n",
    "\n",
    "file2.close()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d944be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "    vectors = vectorizer.fit_transform(file1)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    \n",
    "    with open(\"TFIDFwithngram\", 'w',encoding=\"utf8\") as file2:\n",
    "        for i in range(len(denselist)):\n",
    "            \n",
    "            file2.write(str(i)+\" - \"+ str(denselist[0][i])+ \"\\t\"+ str(denselist[1][i]))\n",
    "            file2.write(\"\\n\")\n",
    "    file2.close()\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93d38ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'comments'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m                 file2\u001b[38;5;241m.\u001b[39mwrite(commentWithNoStopWords)\n\u001b[0;32m     30\u001b[0m                 file2\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mnormlizeComment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormlizedComments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m removeStopWords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormlizedComments\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommentsWithNoStopWords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m read_file \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommentsWithNoStopWords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mnormlizeComment\u001b[1;34m(file1, file2)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormlizeComment\u001b[39m(file1,file2):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file1:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file2:\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m file1:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'comments'"
     ]
    }
   ],
   "source": [
    "from aranorm import normalize_arabic_text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def normlizeComment(file1,file2):\n",
    "\n",
    "    with open(file1, 'r') as file1:\n",
    "        with open(file2, 'w',newline='') as file2:\n",
    "            for row in file1:\n",
    "       \n",
    "       \n",
    "                file2.write(normalize_arabic_text(row))\n",
    "                file2.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "def removeStopWords(file1,file2):\n",
    "\n",
    "    arabicStopWords= stopwords.words(\"arabic\")\n",
    "\n",
    "    with open(file1, 'r') as file1:\n",
    "        with open(file2, 'w',newline='') as file2:\n",
    "\n",
    "            for row in file1:\n",
    "                tokenizedRow = word_tokenize(row)\n",
    "                commentWithNoStopWords= ' '.join([i for i in tokenizedRow if i not in arabicStopWords])\n",
    "                file2.write(commentWithNoStopWords)\n",
    "                file2.write('\\n')\n",
    "\n",
    "normlizeComment('comments','normlizedComments')\n",
    "removeStopWords('normlizedComments','commentsWithNoStopWords')\n",
    "       \n",
    "read_file = pd.read_csv (r'commentsWithNoStopWords')\n",
    "read_file.to_csv (r'cleanComments.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b57c9022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "wordset = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    c=0\n",
    "    for row in file1:\n",
    "        if c<1:\n",
    "            wordset.append(row)\n",
    "            c=c+1\n",
    "       \n",
    "vectorizer =  CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "\n",
    "df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow_sklearn.head()\n",
    "\n",
    "df_bow_sklearn.to_csv('BOWoneComment.csv', encoding='utf-8')\n",
    "\n",
    "file1.close()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d48f2eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([365, 680], dtype=int64),)\n",
      "(array([365, 680], dtype=int64),)\n",
      "(array([1401], dtype=int64),)\n",
      "(950, 2566)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "wordset = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "   \n",
    "    for row in file1:\n",
    "       \n",
    "            wordset.append(row)\n",
    "           \n",
    "\n",
    "        \n",
    "vectorizer =  CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "z=X.toarray()\n",
    "columns=vectorizer.get_feature_names_out()\n",
    "with open(\"BOWoneComment.csv\", 'w',encoding=\"utf8\") as file2:\n",
    "    for i in z[55]:\n",
    "        if i>=1:\n",
    "                print(np.where(z[55]==i))\n",
    "                \n",
    "        file2.write(str(i))\n",
    "    file2.write('\\n')\n",
    "    file2.write('_____________________________________')\n",
    "    file2.write('\\n')\n",
    "    for i in columns:\n",
    "        \n",
    "         file2.write(str(i)+\" \")\n",
    "\n",
    "            \n",
    "\n",
    "#df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "#df_bow_sklearn.head()\n",
    "\n",
    "#df_bow_sklearn.to_csv('BOWoneComment.csv', encoding='utf-8')\n",
    "\n",
    "file1.close()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7533063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يوميين\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         file2\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m file2\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(denselist, columns\u001b[38;5;241m=\u001b[39m\u001b[43mfeature_names\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "    vectors = vectorizer.fit_transform(file1)\n",
    "    feature_tfidf = vectorizer.get_feature_names_out()\n",
    "    max_value = feature_tfidf.max()\n",
    "    print(max_value)\n",
    "  \n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    \n",
    "    with open(\"TFIDFwithngram\", 'w',encoding=\"utf8\") as file2:\n",
    "        for i in range(len(denselist)):\n",
    "            \n",
    "            file2.write(str(i)+\" - \"+ str(denselist[0][i])+ \"\\t\"+ str(denselist[1][i]))\n",
    "            file2.write(\"\\n\")\n",
    "    file2.close()\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a4a4ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>اجمل</th>\n",
       "      <th>الاشتراك</th>\n",
       "      <th>الباقه</th>\n",
       "      <th>الرساءل</th>\n",
       "      <th>تشرحو</th>\n",
       "      <th>دي</th>\n",
       "      <th>رمز</th>\n",
       "      <th>شنو</th>\n",
       "      <th>طريقه</th>\n",
       "      <th>طيب</th>\n",
       "      <th>قللتو</th>\n",
       "      <th>ليها</th>\n",
       "      <th>ميقات</th>\n",
       "      <th>واديتونا</th>\n",
       "      <th>يكون</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268070</td>\n",
       "      <td>0.35248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.35248</td>\n",
       "      <td>0.536140</td>\n",
       "      <td>0.268070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.35248</td>\n",
       "      <td>0.35248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.373801</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.284285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.373801</td>\n",
       "      <td>0.284285</td>\n",
       "      <td>0.373801</td>\n",
       "      <td>0.373801</td>\n",
       "      <td>0.373801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.680919</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       اجمل  الاشتراك   الباقه   الرساءل    تشرحو        دي       رمز  \\\n",
       "0  0.000000  0.268070  0.35248  0.000000  0.35248  0.536140  0.268070   \n",
       "1  0.373801  0.000000  0.00000  0.373801  0.00000  0.284285  0.000000   \n",
       "2  0.000000  0.517856  0.00000  0.000000  0.00000  0.000000  0.517856   \n",
       "\n",
       "        شنو    طريقه      طيب     قللتو      ليها     ميقات  واديتونا  \\\n",
       "0  0.000000  0.35248  0.35248  0.000000  0.268070  0.000000  0.000000   \n",
       "1  0.000000  0.00000  0.00000  0.373801  0.284285  0.373801  0.373801   \n",
       "2  0.680919  0.00000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       يكون  \n",
       "0  0.000000  \n",
       "1  0.373801  \n",
       "2  0.000000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "  \n",
    "        \n",
    "     'طيب تشرحو ليها رمز دي طريقه الاشتراك الباقه دي.',\n",
    "     '.قللتو الرساءل دي واديتونا ليها ميقات يكون اجمل',\n",
    "     '.رمز الاشتراك شنو',\n",
    " ]\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "\n",
    "pd.DataFrame(\n",
    "X.todense(),\n",
    "columns=vectorizer.get_feature_names_out()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6a071f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(950, 2641)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "corpus = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"normlizedComments\", 'r',encoding=\"utf8\") as file1:\n",
    "   \n",
    "    for row in file1:\n",
    "           \n",
    "                corpus.append(row)\n",
    "               \n",
    "        \n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "z=X.toarray()\n",
    "columns=vectorizer.get_feature_names_out()\n",
    "with open(\"TFIDFoneComment.csv\", 'w',encoding=\"utf8\") as file2:\n",
    "    for i in range(len(z)):         \n",
    "        file2.write(str(z[i][0])+\"\\n\")\n",
    "    file2.write('\\n')\n",
    "    file2.write('_____________________________________')\n",
    "    file2.write('\\n')\n",
    "    \n",
    "\n",
    "            \n",
    "print(X.shape)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da7e1376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(950, 2641)\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "corpus = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"normlizedComments\", 'r',encoding=\"utf8\") as file1:\n",
    "   \n",
    "    for row in file1:\n",
    "           \n",
    "                corpus.append(row)\n",
    "               \n",
    "        \n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "z=X.toarray()\n",
    "columns=vectorizer.get_feature_names_out()\n",
    "\n",
    "wb =Workbook() \n",
    "\n",
    "sheet = wb.active \n",
    "for i in range(len(z)):\n",
    "    c = sheet.cell(row =1, column = i+1) \n",
    "    c.value =str(z[i][1])\n",
    "        \n",
    "wb.save(filename=\"tfidfexcel2.xlsx\")\n",
    "    \n",
    "\n",
    "            \n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9dea51a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n",
      "(950, 2641)\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "corpus = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"normlizedComments\", 'r',encoding=\"utf8\") as file1:\n",
    "   \n",
    "    for row in file1:\n",
    "           \n",
    "                corpus.append(row)\n",
    "               \n",
    "        \n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "z=X.toarray()\n",
    "columns=vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "wb =Workbook() \n",
    "\n",
    "sheet = wb.active \n",
    "print(len(z))\n",
    "\n",
    "for j in range(len(z)):\n",
    "    \n",
    "    for i in range(len(z[0])):\n",
    "        \n",
    "        c = sheet.cell(row =i+1, column = j+1) \n",
    "        c.value =str(z[j][i])\n",
    "\n",
    "        \n",
    "wb.save(filename=\"tfidfexcel.xlsx\")\n",
    "    \n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "06ad29e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'tfidfexcel.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m         c \u001b[38;5;241m=\u001b[39m sheet\u001b[38;5;241m.\u001b[39mcell(row \u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, column \u001b[38;5;241m=\u001b[39m j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m     39\u001b[0m         c\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(z[j][i])\n\u001b[1;32m---> 42\u001b[0m \u001b[43mwb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtfidfexcel.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openpyxl\\workbook\\workbook.py:407\u001b[0m, in \u001b[0;36mWorkbook.save\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworksheets:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sheet()\n\u001b[1;32m--> 407\u001b[0m \u001b[43msave_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openpyxl\\writer\\excel.py:291\u001b[0m, in \u001b[0;36msave_workbook\u001b[1;34m(workbook, filename)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_workbook\u001b[39m(workbook, filename):\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;124;03m\"\"\"Save the given workbook on the filesystem under the name filename.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m    :param workbook: the workbook to save\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    289\u001b[0m \n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     archive \u001b[38;5;241m=\u001b[39m \u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZIP_DEFLATED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowZip64\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     writer \u001b[38;5;241m=\u001b[39m ExcelWriter(workbook, archive)\n\u001b[0;32m    293\u001b[0m     writer\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\zipfile.py:1248\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'tfidfexcel.xlsx'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "corpus = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "   \n",
    "    for row in file1:\n",
    "           \n",
    "                corpus.append(row)\n",
    "               \n",
    "        \n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "z=X.toarray()\n",
    "columns=vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "wb =Workbook() \n",
    "\n",
    "sheet = wb.active \n",
    "print(len(z))\n",
    "\n",
    "for j in range(len(z)):\n",
    "    \n",
    "    for i in range(len(z[0])):\n",
    "        \n",
    "        c = sheet.cell(row =i+1, column = j+1) \n",
    "        c.value =str(z[j][i])\n",
    "\n",
    "        \n",
    "wb.save(filename=\"tfidfexcel.xlsx\")\n",
    "    \n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "598650a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Athar/nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Athar/nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                 file2\u001b[38;5;241m.\u001b[39mwrite(commentWithNoStopWords)\n\u001b[0;32m     17\u001b[0m                 file2\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mremoveStopWords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormlizedComments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcommentsWithNoStopWords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [97]\u001b[0m, in \u001b[0;36mremoveStopWords\u001b[1;34m(file1, file2)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremoveStopWords\u001b[39m(file1,file2):\n\u001b[1;32m----> 8\u001b[0m     arabicStopWords\u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marabic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file1:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file2:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Athar/nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Athar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7686cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n",
      "(950, 2566)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "corpus = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "   \n",
    "    for row in file1:\n",
    "           \n",
    "                corpus.append(row)\n",
    "               \n",
    "        \n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "z=X.toarray()\n",
    "columns=vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "wb =Workbook() \n",
    "\n",
    "sheet = wb.active \n",
    "print(len(z))\n",
    "\n",
    "for j in range(len(z)):\n",
    "    \n",
    "    for i in range(len(z[0])):\n",
    "        \n",
    "        c = sheet.cell(row =i+1, column = j+1) \n",
    "        c.value =str(z[j][i])\n",
    "\n",
    "        \n",
    "wb.save(filename=\"tfidfexcelnew.xlsx\")\n",
    "    \n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "corpus = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "   \n",
    "    for row in file1:\n",
    "           \n",
    "                corpus.append(row)\n",
    "               \n",
    "        \n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "z=X.toarray()\n",
    "columns=vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "wb =Workbook() \n",
    "\n",
    "sheet = wb.active \n",
    "print(len(z))\n",
    "\n",
    "for j in range(len(z)):\n",
    "    \n",
    "    for i in range(len(z[0])):\n",
    "        \n",
    "        c = sheet.cell(row =i+1, column = j+1) \n",
    "        c.value =str(z[j][i])\n",
    "\n",
    "        \n",
    "wb.save(filename=\"tfidfexcelnew.xlsx\")\n",
    "    \n",
    "print(X.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
