{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9927792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['اجمل' 'الاشتراك' 'الباقه' 'الرساءل' 'تشرحو' 'دي' 'رمز' 'شنو' 'طريقه'\n",
      " 'طيب' 'قللتو' 'ليها' 'ميقات' 'واديتونا' 'يكون']\n",
      "(3, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "  \n",
    "        \n",
    "     'طيب تشرحو ليها رمز دي طريقه الاشتراك الباقه دي.',\n",
    "     '.قللتو الرساءل دي واديتونا ليها ميقات يكون اجمل',\n",
    "     '.رمز الاشتراك شنو',\n",
    " ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a934e3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'an' 'best' 'game' 'great' 'is' 'of' 'series' 'so' 'the'\n",
      " 'thrones' 'tv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "\n",
    "wordset =[\n",
    "   'Game of Thrones is an amazing tv series!',\n",
    "   'Game of Thrones is the best tv series!',\n",
    "   'Game of Thrones is so great',\n",
    "]\n",
    "\n",
    "\n",
    "l_doc1 = re.sub(r\"[^a-zA-Z0-9]\", \" \", doc1.lower()).split()\n",
    "l_doc2 = re.sub(r\"[^a-zA-Z0-9]\", \" \", doc2.lower()).split()\n",
    "l_doc3 = re.sub(r\"[^a-zA-Z0-9]\", \" \", doc3.lower()).split()\n",
    "\n",
    "\n",
    "def calculateBOW(wordset,l_doc):\n",
    "  tf_diz = dict.fromkeys(' ',\"sen\")\n",
    "\n",
    "  for word in l_doc:\n",
    "      tf_diz[word]=l_doc.count(word)\n",
    "\n",
    "  return tf_diz\n",
    "\n",
    "bow1 = calculateBOW(wordset,l_doc1)\n",
    "bow2 = calculateBOW(wordset,l_doc2)\n",
    "bow3 = calculateBOW(wordset,l_doc3)\n",
    "df_bow = pd.DataFrame([bow1,bow2,bow3])\n",
    "df_bow.head()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6346a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 270)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "wordset = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    c=0\n",
    "    for row in file1:\n",
    "        if c<50:\n",
    "            wordset.append(row)\n",
    "            c=c+1\n",
    "       \n",
    "vectorizer =  CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "\n",
    "df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow_sklearn.head()\n",
    "\n",
    "df_bow_sklearn.to_csv('BOW2.csv', encoding='utf-8')\n",
    "\n",
    "file2.close()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ddd0e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 270)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "wordset = [\n",
    "            \n",
    "     \n",
    "\n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    c=0\n",
    "    for row in file1:\n",
    "        if c<50:\n",
    "            wordset.append(row)\n",
    "            c=c+1\n",
    "       \n",
    "vectorizer =  CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "\n",
    "df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow_sklearn.head()\n",
    "\n",
    "df_bow_sklearn.to_csv('BOW3.csv', encoding='utf-8')\n",
    "\n",
    "file2.close()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761d2256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1921)\t0.4792561516861785\n",
      "  (0, 756)\t0.43288847457257656\n",
      "  (0, 353)\t0.313029767988369\n",
      "  (0, 995)\t0.4521327993291748\n",
      "  (0, 2562)\t0.3051507403879635\n",
      "  (0, 2156)\t0.43288847457257656\n",
      "  (1, 1234)\t0.23923718083170847\n",
      "  (1, 219)\t0.4103235280645333\n",
      "  (1, 189)\t0.3877088831278344\n",
      "  (1, 1521)\t0.4437003239871909\n",
      "  (1, 937)\t0.5087697125207767\n",
      "  (1, 1531)\t0.4103235280645333\n",
      "  (2, 30)\t0.36106094840913283\n",
      "  (2, 2548)\t0.3298372507973552\n",
      "  (2, 2093)\t0.39973501449669907\n",
      "  (2, 1876)\t0.32238688232156665\n",
      "  (2, 2207)\t0.39973501449669907\n",
      "  (2, 330)\t0.37711213608506106\n",
      "  (2, 1719)\t0.39973501449669907\n",
      "  (2, 1234)\t0.1879661379096089\n",
      "  (3, 1457)\t0.45111455557071134\n",
      "  (3, 1296)\t0.6942606978229787\n",
      "  (3, 189)\t0.5608009818202802\n",
      "  (5, 189)\t1.0\n",
      "  (6, 1871)\t0.38998829851435735\n",
      "  :\t:\n",
      "  (942, 1952)\t0.7697980646403396\n",
      "  (942, 1157)\t0.5772768001053054\n",
      "  (942, 1394)\t0.27232780933310335\n",
      "  (943, 1246)\t0.7697980646403396\n",
      "  (943, 1157)\t0.5772768001053054\n",
      "  (943, 1394)\t0.27232780933310335\n",
      "  (944, 407)\t0.7282187490288227\n",
      "  (944, 308)\t0.6350828025448557\n",
      "  (944, 1394)\t0.25761849210541654\n",
      "  (945, 198)\t0.8294867818462057\n",
      "  (945, 365)\t0.4638979446287559\n",
      "  (945, 1394)\t0.31104722424680353\n",
      "  (946, 437)\t0.6859701534812822\n",
      "  (946, 510)\t0.6859701534812822\n",
      "  (946, 1394)\t0.24267240688988992\n",
      "  (947, 1417)\t0.9363330070353189\n",
      "  (947, 1394)\t0.3511132295089398\n",
      "  (948, 1539)\t0.46542654467800737\n",
      "  (948, 971)\t0.46542654467800737\n",
      "  (948, 904)\t0.46542654467800737\n",
      "  (948, 614)\t0.43908587461422366\n",
      "  (948, 1081)\t0.3608708820776578\n",
      "  (948, 1394)\t0.16465174068326116\n",
      "  (949, 653)\t0.7347435161440878\n",
      "  (949, 1872)\t0.6783450195027767\n",
      "['ءاجترار' 'االلهم' 'االنت' ... 'يومي' 'يومين' 'يوميين']\n",
      "(950, 2566)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "wordset = [\n",
    "            \n",
    "]\n",
    "\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "\n",
    "    for row in file1:\n",
    "        wordset.append(row)\n",
    "       \n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(wordset)\n",
    "print(X)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "with open(\"commentsWithTFIDF\", 'w',encoding=\"utf8\") as file2:\n",
    "    file2.write(str(result))\n",
    "file2.close()\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12462cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(file1)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    \n",
    "    with open(\"TFIDFDENSE\", 'w',encoding=\"utf8\") as file2:\n",
    "        for i in range(len(denselist)):\n",
    "            \n",
    "            file2.write(str(i)+\" - \"+ str(denselist[0][i])+ \"\\t\"+ str(denselist[1][i]))\n",
    "            file2.write(\"\\n\")\n",
    "    file2.close()\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d1257ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(file1)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    \n",
    "    with open(\"feature_names\", 'w',encoding=\"utf8\") as file2:\n",
    "        for i in feature_names:\n",
    "            file2.write(str(i+\" \\n\"))\n",
    "    file2.close()\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open(\"commentsWithNoStopWords\", 'r',encoding=\"utf8\") as file1:\n",
    "    with open(\"feature_names\", 'r',encoding=\"utf8\") as file2:\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectors = vectorizer.fit_transform([file1,file2])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        \n",
    "        \n",
    "        with open(\"TFIDFwithfeture\", 'w',encoding=\"utf8\") as file3:\n",
    "            for i in range(len(denselist)):\n",
    "\n",
    "                file3.write(str(i)+\" - \"+ str(denselist[0][i])+ \"\\t\"+ str(denselist[1][i]))\n",
    "                file3.write(\"\\n\")\n",
    "        file3.close()\n",
    "    \n",
    "\n",
    "\n",
    "        df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
