{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45dd141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from aranorm import normalize_arabic_text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "rand_seed = 0  # random state for reproducibility\n",
    "np.random.seed(rand_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb466384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "      <td>محايد</td>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "      <td>محايد</td>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>واو</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment   label  \\\n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال    سلبي   \n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي   محايد   \n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي   \n",
       "3                                   رمز الاشتراك شنو   محايد   \n",
       "4                                                واو  ايجابي   \n",
       "\n",
       "                                               clean  \n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال  \n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي  \n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...  \n",
       "3                                   رمز الاشتراك شنو  \n",
       "4                                                واو  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_excel('ALL_data.xlsx')\n",
    "data = data.dropna()\n",
    "data['clean']=data['comment']\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f49707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 04:22:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8832e1f794d94d3983f46a3d91c62cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 04:22:53 WARNING: Language ar package default expects mwt, which has been added\n",
      "2023-01-05 04:22:53 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| lemma     | padt    |\n",
      "=======================\n",
      "\n",
      "2023-01-05 04:22:53 INFO: Use device: cpu\n",
      "2023-01-05 04:22:53 INFO: Loading: tokenize\n",
      "2023-01-05 04:22:53 INFO: Loading: mwt\n",
      "2023-01-05 04:22:53 INFO: Loading: lemma\n",
      "2023-01-05 04:22:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline(lang='ar', processors='tokenize,lemma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21bccfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arabicStopWords= stopwords.words(\"arabic\")\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    tokenizedRow = word_tokenize(data['clean'][i])\n",
    "    commentWithNoStopWords= ' '.join([i for i in tokenizedRow if i not in arabicStopWords])\n",
    "                \n",
    "\n",
    "    data['clean'][i]=commentWithNoStopWords\n",
    "    \n",
    "\n",
    "for i in range(0,len(data)):\n",
    "\n",
    "    data['clean'][i]=normalize_arabic_text(data['clean'][i])\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    doc = nlp(data['clean'][i])\n",
    "    lema=''.join(word.lemma+' ' for sent in doc.sentences for word in sent.words)\n",
    "\n",
    "    data['clean'][i]=lema\n",
    "\n",
    "for i in range(0,len(data)):  \n",
    "    data['clean'][i]=normalize_arabic_text(data['clean'][i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d287350a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>نفسي يوم تكتب سعر بدون مانسال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>قلل رساءل دي وادي لي هو ميقات كان اجمل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>شكر توضيح مفيد اكرر شكر سوداني ابداع تميز</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>سوداني جميل</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>سوداني جميل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>خليك سوداني</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>خليك سوداني</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>سوداني</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>سوداني</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>سوداني الاقوي والافضل</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>سوداني اقوي وافل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>خليك سوداني</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>خليك سوداني</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>سوداني</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>سوداني</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2374 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment   label  \\\n",
       "0                      نفسي يوم تكتبو السعر بدون مانسال    سلبي   \n",
       "2      لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي   \n",
       "4                                                   واو  ايجابي   \n",
       "8     شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...  ايجابي   \n",
       "13                                          سوداني جميل  ايجابي   \n",
       "...                                                 ...     ...   \n",
       "3068                                        خليك سوداني  ايجابي   \n",
       "3069                                             سوداني  ايجابي   \n",
       "3070                              سوداني الاقوي والافضل  ايجابي   \n",
       "3071                                        خليك سوداني  ايجابي   \n",
       "3072                                             سوداني  ايجابي   \n",
       "\n",
       "                                          clean  \n",
       "0                 نفسي يوم تكتب سعر بدون مانسال  \n",
       "2        قلل رساءل دي وادي لي هو ميقات كان اجمل  \n",
       "4                                                \n",
       "8     شكر توضيح مفيد اكرر شكر سوداني ابداع تميز  \n",
       "13                                  سوداني جميل  \n",
       "...                                         ...  \n",
       "3068                                خليك سوداني  \n",
       "3069                                     سوداني  \n",
       "3070                           سوداني اقوي وافل  \n",
       "3071                                خليك سوداني  \n",
       "3072                                     سوداني  \n",
       "\n",
       "[2374 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['label'] != 'محايد']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4da95129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'clean']\n",
      "output: label\n",
      "features: ['clean']\n",
      "train data = 1899\n",
      "val  data = 237\n",
      "test  data = 238\n",
      "all data = 2374\n",
      "1899\n",
      "237\n",
      "238\n",
      "2374\n",
      "2374\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.9062664560294892\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.9113924050632911\n",
      "f1_score  on test data:\n",
      "0.8912487708947887\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.9652448657187994\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.919831223628692\n",
      "f1_score  on test data:\n",
      "0.9024670233273409\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.9921011058451816\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.9451476793248945\n",
      "f1_score  on test data:\n",
      "0.9354155748873283\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.9978936282253817\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.9029535864978903\n",
      "f1_score  on test data:\n",
      "0.8882373444323703\n",
      "Iteration 1, loss = 0.73185997\n",
      "Iteration 2, loss = 0.70488290\n",
      "Iteration 3, loss = 0.68148105\n",
      "Iteration 4, loss = 0.65754886\n",
      "Iteration 5, loss = 0.62969668\n",
      "Iteration 6, loss = 0.59333998\n",
      "Iteration 7, loss = 0.54710751\n",
      "Iteration 8, loss = 0.48499173\n",
      "Iteration 9, loss = 0.40155232\n",
      "Iteration 10, loss = 0.30264147\n",
      "Iteration 11, loss = 0.20896150\n",
      "Iteration 12, loss = 0.13632054\n",
      "Iteration 13, loss = 0.08511272\n",
      "Iteration 14, loss = 0.05068924\n",
      "Iteration 15, loss = 0.03120555\n",
      "Iteration 16, loss = 0.02010140\n",
      "Iteration 17, loss = 0.01390142\n",
      "Iteration 18, loss = 0.01045996\n",
      "Iteration 19, loss = 0.00857190\n",
      "Iteration 20, loss = 0.00705155\n",
      "Iteration 21, loss = 0.00628588\n",
      "Iteration 22, loss = 0.00562990\n",
      "Iteration 23, loss = 0.00512739\n",
      "Iteration 24, loss = 0.00512917\n",
      "Iteration 25, loss = 0.00472197\n",
      "Iteration 26, loss = 0.00443342\n",
      "Iteration 27, loss = 0.00435432\n",
      "Iteration 28, loss = 0.00418030\n",
      "Iteration 29, loss = 0.00417362\n",
      "Iteration 30, loss = 0.00426456\n",
      "Iteration 31, loss = 0.00409903\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9978936282253817\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.9324894514767933\n",
      "f1_score  on test data:\n",
      "0.9225237005557372\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "features.remove('comment')\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "\n",
    "print(\"train data = \"+str(len(train_data)))\n",
    "print(\"val  data = \"+str(len(val_data)))\n",
    "print(\"test  data = \"+str(len(test_data)))\n",
    "\n",
    "print(\"all data = \"+str(len(data)))\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "print(len(train_data)+len(val_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TF IDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['clean'])\n",
    "val_data_features = vectorizer.transform(val_data['clean'])\n",
    "test_data_features = vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, val_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    val_data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    val_data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "# RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eb61115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# غير محايد"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9aba288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment   label\n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال    سلبي\n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي   محايد\n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي\n",
       "3                                   رمز الاشتراك شنو   محايد\n",
       "4                                                واو  ايجابي"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading our prepared data\n",
    "data = pd.read_excel('ALL_data.xlsx')\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74de57d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "      <td>محايد</td>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "      <td>محايد</td>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>واو</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment   label  \\\n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال    سلبي   \n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي   محايد   \n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي   \n",
       "3                                   رمز الاشتراك شنو   محايد   \n",
       "4                                                واو  ايجابي   \n",
       "\n",
       "                                               clean  \n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال  \n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي  \n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...  \n",
       "3                                   رمز الاشتراك شنو  \n",
       "4                                                واو  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('label').count()\n",
    "data['clean']=data['comment']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fac3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arabicStopWords= stopwords.words(\"arabic\")\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    tokenizedRow = word_tokenize(data['clean'][i])\n",
    "    commentWithNoStopWords= ' '.join([i for i in tokenizedRow if i not in arabicStopWords])\n",
    "                \n",
    "\n",
    "    data['clean'][i]=commentWithNoStopWords\n",
    "    \n",
    "\n",
    "for i in range(0,len(data)):\n",
    "\n",
    "    data['clean'][i]=normalize_arabic_text(data['clean'][i])\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    doc = nlp(data['clean'][i])\n",
    "    lema=''.join(word.lemma+' ' for sent in doc.sentences for word in sent.words)\n",
    "\n",
    "    data['clean'][i]=lema\n",
    "\n",
    "for i in range(0,len(data)):  \n",
    "    data['clean'][i]=normalize_arabic_text(data['clean'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93900c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ايجابي</th>\n",
       "      <td>773</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>سلبي</th>\n",
       "      <td>1601</td>\n",
       "      <td>1601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>محايد</th>\n",
       "      <td>699</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        comment  clean\n",
       "label                 \n",
       "ايجابي      773    773\n",
       "سلبي       1601   1601\n",
       "محايد       699    699"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5a49ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(773, 1601, 699)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data = data[data['label'] == 'ايجابي'].dropna()\n",
    "negative_data = data[data['label'] == 'سلبي'].dropna()\n",
    "neutral_data = data[data['label'] == 'محايد'].dropna()\n",
    "len(positive_data), len(negative_data), len(neutral_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21398233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabab\\AppData\\Local\\Temp\\ipykernel_5996\\1779321054.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  non_neutral_data = positive_data.append(negative_data).sample(frac=1).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "non_neutral_data = positive_data.append(negative_data).sample(frac=1).reset_index(drop=True)\n",
    "non_neutral_data['label'] = 'غير محايد'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdc284dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabab\\AppData\\Local\\Temp\\ipykernel_5996\\300981967.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  neu_data = neutral_data.append(non_neutral_data).dropna().sample(frac=1).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الناس تتفق تقفل يومين بس لانت لامكالمات والله ...</td>\n",
       "      <td>غير محايد</td>\n",
       "      <td>انسان اتفق تقفل يوم لانت ل امكالم و الل ه يقال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>انتو يا جماعه شركه كنداكه ده وين اختفي كده ولا...</td>\n",
       "      <td>محايد</td>\n",
       "      <td>انتو جماع هو شرك هو كنداك هو ده وينه اختفي كده...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>وين السعر وانا اقول العلم التجار حركه عدم الاس...</td>\n",
       "      <td>غير محايد</td>\n",
       "      <td>و ين سعر و هو اقول علم تاجر حرك هو عدم اسعارمي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>والشبكه زفت الزفت</td>\n",
       "      <td>غير محايد</td>\n",
       "      <td>و شبكه زفت زفت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>بالغتو والله طلعتو زيتنا تسقطو بس</td>\n",
       "      <td>غير محايد</td>\n",
       "      <td>ب غتو و الل ه طلع زيه هو تسقط</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>والله حرام عليكم زيادة فظيعة حتى التي قبلها لم...</td>\n",
       "      <td>غير محايد</td>\n",
       "      <td>و الل ه حرام علي هو زيادا هو فظيع هو قبل هو نش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>1قيقا</td>\n",
       "      <td>محايد</td>\n",
       "      <td>قيقا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>عمل انساني شنو المناقل ماشايفنها</td>\n",
       "      <td>محايد</td>\n",
       "      <td>عمل انساني شنا مناقل ماشا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>كل يومين تلاتة زايدين اسعاركم</td>\n",
       "      <td>غير محايد</td>\n",
       "      <td>يوم تلات هو زايد اسعاركي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>شركة زي الزفت فجأة بتلقى نفسك مشترك في خدمات م...</td>\n",
       "      <td>غير محايد</td>\n",
       "      <td>شركه زي زفت فجاه ب تلقي نفس ك مشترك خدمه ب عرف...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3073 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment      label  \\\n",
       "0     الناس تتفق تقفل يومين بس لانت لامكالمات والله ...  غير محايد   \n",
       "1     انتو يا جماعه شركه كنداكه ده وين اختفي كده ولا...      محايد   \n",
       "2     وين السعر وانا اقول العلم التجار حركه عدم الاس...  غير محايد   \n",
       "3                                     والشبكه زفت الزفت  غير محايد   \n",
       "4                     بالغتو والله طلعتو زيتنا تسقطو بس  غير محايد   \n",
       "...                                                 ...        ...   \n",
       "3068  والله حرام عليكم زيادة فظيعة حتى التي قبلها لم...  غير محايد   \n",
       "3069                                              1قيقا      محايد   \n",
       "3070                   عمل انساني شنو المناقل ماشايفنها      محايد   \n",
       "3071                      كل يومين تلاتة زايدين اسعاركم  غير محايد   \n",
       "3072  شركة زي الزفت فجأة بتلقى نفسك مشترك في خدمات م...  غير محايد   \n",
       "\n",
       "                                                  clean  \n",
       "0     انسان اتفق تقفل يوم لانت ل امكالم و الل ه يقال...  \n",
       "1     انتو جماع هو شرك هو كنداك هو ده وينه اختفي كده...  \n",
       "2     و ين سعر و هو اقول علم تاجر حرك هو عدم اسعارمي...  \n",
       "3                                        و شبكه زفت زفت  \n",
       "4                         ب غتو و الل ه طلع زيه هو تسقط  \n",
       "...                                                 ...  \n",
       "3068  و الل ه حرام علي هو زيادا هو فظيع هو قبل هو نش...  \n",
       "3069                                               قيقا  \n",
       "3070                          عمل انساني شنا مناقل ماشا  \n",
       "3071                           يوم تلات هو زايد اسعاركي  \n",
       "3072  شركه زي زفت فجاه ب تلقي نفس ك مشترك خدمه ب عرف...  \n",
       "\n",
       "[3073 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neu_data = neutral_data.append(non_neutral_data).dropna().sample(frac=1).reset_index(drop=True)\n",
    "neu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5eeb9d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'clean']\n",
      "output: label\n",
      "features: ['clean']\n",
      "train data = 2458\n",
      "val  data = 307\n",
      "test  data = 308\n",
      "all data = 3073\n",
      "2458\n",
      "307\n",
      "308\n",
      "3073\n",
      "3073\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.9202603742880391\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.758957654723127\n",
      "f1_score  on test data:\n",
      "0.7015155479546422\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.8641171684296176\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7296416938110749\n",
      "f1_score  on test data:\n",
      "0.6154767325380203\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.9703010577705452\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7785016286644951\n",
      "f1_score  on test data:\n",
      "0.7381975105036299\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.9894222945484134\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.742671009771987\n",
      "f1_score  on test data:\n",
      "0.7095633059260904\n",
      "Iteration 1, loss = 1.04466199\n",
      "Iteration 2, loss = 1.01828181\n",
      "Iteration 3, loss = 0.98089249\n",
      "Iteration 4, loss = 0.91640689\n",
      "Iteration 5, loss = 0.82730399\n",
      "Iteration 6, loss = 0.71287437\n",
      "Iteration 7, loss = 0.58290943\n",
      "Iteration 8, loss = 0.47045638\n",
      "Iteration 9, loss = 0.37954316\n",
      "Iteration 10, loss = 0.29801554\n",
      "Iteration 11, loss = 0.22487338\n",
      "Iteration 12, loss = 0.15896686\n",
      "Iteration 13, loss = 0.10733301\n",
      "Iteration 14, loss = 0.07296903\n",
      "Iteration 15, loss = 0.05335375\n",
      "Iteration 16, loss = 0.04196032\n",
      "Iteration 17, loss = 0.03562776\n",
      "Iteration 18, loss = 0.03178110\n",
      "Iteration 19, loss = 0.02880052\n",
      "Iteration 20, loss = 0.02727980\n",
      "Iteration 21, loss = 0.02538865\n",
      "Iteration 22, loss = 0.02414052\n",
      "Iteration 23, loss = 0.02287718\n",
      "Iteration 24, loss = 0.02337967\n",
      "Iteration 25, loss = 0.02251203\n",
      "Iteration 26, loss = 0.02268795\n",
      "Iteration 27, loss = 0.02254885\n",
      "Iteration 28, loss = 0.02243943\n",
      "Iteration 29, loss = 0.02302233\n",
      "Iteration 30, loss = 0.02181703\n",
      "Iteration 31, loss = 0.02144788\n",
      "Iteration 32, loss = 0.02096966\n",
      "Iteration 33, loss = 0.02101718\n",
      "Iteration 34, loss = 0.02172983\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9894222945484134\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8110749185667753\n",
      "f1_score  on test data:\n",
      "0.790920245398773\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "features.remove('comment')\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "\n",
    "print(\"train data = \"+str(len(train_data)))\n",
    "print(\"val  data = \"+str(len(val_data)))\n",
    "print(\"test  data = \"+str(len(test_data)))\n",
    "\n",
    "print(\"all data = \"+str(len(data)))\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "print(len(train_data)+len(val_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TF IDF\n",
    "\n",
    "\n",
    "neu_vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = neu_vectorizer.fit_transform(train_data['clean'])\n",
    "val_data_features = neu_vectorizer.transform(val_data['clean'])\n",
    "test_data_features = neu_vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, val_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    val_data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    val_data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "neu_logistic_reg  = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(neu_logistic_reg , train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "neu_mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(neu_mnb, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# S V M\n",
    "\n",
    "neu_svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(neu_svm, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "# RandomForestClassifier\n",
    "\n",
    "neu_rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(neu_rf, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "neu_mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(neu_mlp, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "246b1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(f'vectorizer.pkl', 'wb'))\n",
    "pickle.dump(logistic_reg, open(f'logistic_reg.pkl', 'wb'))\n",
    "pickle.dump(mnb, open(f'mnb.pkl', 'wb'))\n",
    "pickle.dump(svm, open(f'svm.pkl', 'wb'))\n",
    "pickle.dump(rf, open(f'rf.pkl', 'wb'))\n",
    "pickle.dump(mlp, open(f'mlp.pkl', 'wb'))\n",
    "pickle.dump(neu_vectorizer, open(f'neu_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(neu_logistic_reg, open(f'neu_logistic_reg.pkl', 'wb'))\n",
    "pickle.dump(neu_mnb, open(f'neu_mnb.pkl', 'wb'))\n",
    "pickle.dump(neu_svm, open(f'neu_svm.pkl', 'wb'))\n",
    "pickle.dump(neu_rf, open(f'neu_rf.pkl', 'wb'))\n",
    "pickle.dump(neu_mlp, open(f'neu_mlp.pkl', 'wb'))\n",
    "vectorizer = pickle.load(open(f'vectorizer.pkl', 'rb'))\n",
    "logistic_reg = pickle.load(open(f'logistic_reg.pkl', 'rb'))\n",
    "mnb = pickle.load(open(f'mnb.pkl', 'rb'))\n",
    "svm = pickle.load(open(f'svm.pkl', 'rb'))\n",
    "rf = pickle.load(open(f'rf.pkl', 'rb'))\n",
    "mlp = pickle.load(open(f'mlp.pkl', 'rb'))\n",
    "\n",
    "neu_vectorizer = pickle.load(open(f'neu_vectorizer.pkl', 'rb'))\n",
    "neu_logistic_reg = pickle.load(open(f'neu_logistic_reg.pkl', 'rb'))\n",
    "neu_mnb = pickle.load(open(f'neu_mnb.pkl', 'rb'))\n",
    "neu_svm = pickle.load(open(f'neu_svm.pkl', 'rb'))\n",
    "neu_rf = pickle.load(open(f'neu_rf.pkl', 'rb'))\n",
    "neu_mlp = pickle.load(open(f'neu_mlp.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4d0db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multi_level(X, neu_vectorizer, neu_clf, vectorizer, clf):\n",
    "    neu_y_pred = neu_clf.predict(neu_vectorizer.transform(X))\n",
    "    if len(X[neu_y_pred == 'غير محايد']) > 0:\n",
    "        y_pred = clf.predict(vectorizer.transform(X[neu_y_pred == 'غير محايد'])) # classify non neutral into positive or negative\n",
    "        neu_y_pred[neu_y_pred == 'غير محايد'] = y_pred\n",
    "    \n",
    "    final_y_pred = neu_y_pred\n",
    "    return final_y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31c6d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = test_data.dropna()['clean'].values\n",
    "y = test_data.dropna()['label'].values\n",
    "pred_y = predict_multi_level(X, neu_vectorizer, neu_mlp, vectorizer, mnb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8715e7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: \n",
      "0.7564935064935064\n",
      "f1_score: \n",
      "0.7299736708483554\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score: ')\n",
    "print(accuracy_score(y, pred_y))\n",
    "\n",
    "print('f1_score: ')\n",
    "print(f1_score(y, pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b3851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
