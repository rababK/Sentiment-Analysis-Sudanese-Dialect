{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3bd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "rand_seed = 0  # random state for reproducibility\n",
    "np.random.seed(rand_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529adb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment   label\n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال    سلبي\n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي   محايد\n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي\n",
       "3                                   رمز الاشتراك شنو   محايد\n",
       "4                                                واو  ايجابي"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('ALL_data.xlsx')\n",
    "data = data.dropna()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ec8a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>سوداني جميل</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>خليك سوداني</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>سوداني</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>سوداني الاقوي والافضل</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>خليك سوداني</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>سوداني</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2374 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment   label\n",
       "0                      نفسي يوم تكتبو السعر بدون مانسال    سلبي\n",
       "2      لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي\n",
       "4                                                   واو  ايجابي\n",
       "8     شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...  ايجابي\n",
       "13                                          سوداني جميل  ايجابي\n",
       "...                                                 ...     ...\n",
       "3068                                        خليك سوداني  ايجابي\n",
       "3069                                             سوداني  ايجابي\n",
       "3070                              سوداني الاقوي والافضل  ايجابي\n",
       "3071                                        خليك سوداني  ايجابي\n",
       "3072                                             سوداني  ايجابي\n",
       "\n",
       "[2374 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['label'] != 'محايد']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3145672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92dbd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment']\n",
      "1899\n",
      "237\n",
      "238\n",
      "2374\n",
      "2374\n"
     ]
    }
   ],
   "source": [
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "print(len(train_data)+len(val_data)+len(test_data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d28e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['comment'].values.astype('U'))\n",
    "val_data_features = vectorizer.transform(val_data['comment'].values.astype('U'))\n",
    "test_data_features = vectorizer.transform(test_data['comment'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac750aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1899, 14373), (237, 14373), (238, 14373))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape, val_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddd5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "    print(\"score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    print('accuracy_score: ')\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print('f1_score: ')\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f95c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on training data:\n",
      "0.8836229594523434\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.890295358649789\n",
      "f1_score: \n",
      "0.8647497805092187\n"
     ]
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f9ff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on training data:\n",
      "0.9694576092680358\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.9071729957805907\n",
      "f1_score: \n",
      "0.8875517598343685\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36736cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on training data:\n",
      "0.9963138493944181\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.9156118143459916\n",
      "f1_score: \n",
      "0.9002525252525253\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e556899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on training data:\n",
      "0.9994734070563455\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.9071729957805907\n",
      "f1_score: \n",
      "0.8911209488807217\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14dd14d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67799087\n",
      "Iteration 2, loss = 0.65454975\n",
      "Iteration 3, loss = 0.63064843\n",
      "Iteration 4, loss = 0.60176048\n",
      "Iteration 5, loss = 0.56629748\n",
      "Iteration 6, loss = 0.51329382\n",
      "Iteration 7, loss = 0.43338528\n",
      "Iteration 8, loss = 0.34852978\n",
      "Iteration 9, loss = 0.26967602\n",
      "Iteration 10, loss = 0.20255696\n",
      "Iteration 11, loss = 0.15397687\n",
      "Iteration 12, loss = 0.11927121\n",
      "Iteration 13, loss = 0.09180937\n",
      "Iteration 14, loss = 0.06936450\n",
      "Iteration 15, loss = 0.05015160\n",
      "Iteration 16, loss = 0.03498275\n",
      "Iteration 17, loss = 0.02393390\n",
      "Iteration 18, loss = 0.01634913\n",
      "Iteration 19, loss = 0.01162851\n",
      "Iteration 20, loss = 0.00850964\n",
      "Iteration 21, loss = 0.00656197\n",
      "Iteration 22, loss = 0.00535723\n",
      "Iteration 23, loss = 0.00443225\n",
      "Iteration 24, loss = 0.00368428\n",
      "Iteration 25, loss = 0.00333799\n",
      "Iteration 26, loss = 0.00307049\n",
      "Iteration 27, loss = 0.00280107\n",
      "Iteration 28, loss = 0.00241295\n",
      "Iteration 29, loss = 0.00222765\n",
      "Iteration 30, loss = 0.00209399\n",
      "Iteration 31, loss = 0.00219830\n",
      "Iteration 32, loss = 0.00190592\n",
      "Iteration 33, loss = 0.00184863\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "score on training data:\n",
      "0.9994734070563455\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.919831223628692\n",
      "f1_score: \n",
      "0.908929113156032\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01aa7315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment   label\n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال    سلبي\n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي   محايد\n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي\n",
       "3                                   رمز الاشتراك شنو   محايد\n",
       "4                                                واو  ايجابي"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading our prepared data\n",
    "data = pd.read_excel('ALL_data.xlsx')\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "294f8f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ايجابي</th>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>سلبي</th>\n",
       "      <td>1601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>محايد</th>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        comment\n",
       "label          \n",
       "ايجابي      773\n",
       "سلبي       1601\n",
       "محايد       699"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e50d6020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(773, 1601, 699)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data = data[data['label'] == 'ايجابي'].dropna()\n",
    "negative_data = data[data['label'] == 'سلبي'].dropna()\n",
    "neutral_data = data[data['label'] == 'محايد'].dropna()\n",
    "len(positive_data), len(negative_data), len(neutral_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03eed4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabab\\AppData\\Local\\Temp\\ipykernel_5536\\1779321054.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  non_neutral_data = positive_data.append(negative_data).sample(frac=1).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "non_neutral_data = positive_data.append(negative_data).sample(frac=1).reset_index(drop=True)\n",
    "non_neutral_data['label'] = 'غير محايد'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1753bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabab\\AppData\\Local\\Temp\\ipykernel_5536\\300981967.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  neu_data = neutral_data.append(non_neutral_data).dropna().sample(frac=1).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الناس تتفق تقفل يومين بس لانت لامكالمات والله ...</td>\n",
       "      <td>غير محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>انتو يا جماعه شركه كنداكه ده وين اختفي كده ولا...</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>وين السعر وانا اقول العلم التجار حركه عدم الاس...</td>\n",
       "      <td>غير محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>والشبكه زفت الزفت</td>\n",
       "      <td>غير محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>بالغتو والله طلعتو زيتنا تسقطو بس</td>\n",
       "      <td>غير محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>والله حرام عليكم زيادة فظيعة حتى التي قبلها لم...</td>\n",
       "      <td>غير محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>1قيقا</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>عمل انساني شنو المناقل ماشايفنها</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>كل يومين تلاتة زايدين اسعاركم</td>\n",
       "      <td>غير محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>شركة زي الزفت فجأة بتلقى نفسك مشترك في خدمات م...</td>\n",
       "      <td>غير محايد</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3073 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment      label\n",
       "0     الناس تتفق تقفل يومين بس لانت لامكالمات والله ...  غير محايد\n",
       "1     انتو يا جماعه شركه كنداكه ده وين اختفي كده ولا...      محايد\n",
       "2     وين السعر وانا اقول العلم التجار حركه عدم الاس...  غير محايد\n",
       "3                                     والشبكه زفت الزفت  غير محايد\n",
       "4                     بالغتو والله طلعتو زيتنا تسقطو بس  غير محايد\n",
       "...                                                 ...        ...\n",
       "3068  والله حرام عليكم زيادة فظيعة حتى التي قبلها لم...  غير محايد\n",
       "3069                                              1قيقا      محايد\n",
       "3070                   عمل انساني شنو المناقل ماشايفنها      محايد\n",
       "3071                      كل يومين تلاتة زايدين اسعاركم  غير محايد\n",
       "3072  شركة زي الزفت فجأة بتلقى نفسك مشترك في خدمات م...  غير محايد\n",
       "\n",
       "[3073 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neu_data = neutral_data.append(non_neutral_data).dropna().sample(frac=1).reset_index(drop=True)\n",
    "neu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfd1c75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment']\n",
      "2458\n",
      "307\n",
      "308\n",
      "3073\n",
      "3073\n"
     ]
    }
   ],
   "source": [
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "# seed = 0  # random state for reproducibility\n",
    "output = 'label' # output label column\n",
    "features = neu_data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "neu_train_data, neu_tmp = random_split(neu_data, features, output, train_fraction, rand_seed)\n",
    "neu_val_data, neu_test_data = random_split(neu_tmp, features, output, val_fraction, rand_seed)\n",
    "\n",
    "print(len(neu_train_data))\n",
    "print(len(neu_val_data))\n",
    "print(len(neu_test_data))\n",
    "print(len(neu_train_data)+len(neu_val_data)+len(neu_test_data))\n",
    "print(len(neu_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b509cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF IDF\n",
    "neu_vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "neu_train_data_features = neu_vectorizer.fit_transform(neu_train_data['comment'].values.astype('U'))\n",
    "neu_val_data_features = neu_vectorizer.transform(neu_val_data['comment'].values.astype('U'))\n",
    "neu_test_data_features = neu_vectorizer.transform(neu_test_data['comment'].values.astype('U'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c830154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on training data:\n",
      "0.8112286411716843\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.7882736156351792\n",
      "f1_score: \n",
      "0.5278599313853071\n"
     ]
    }
   ],
   "source": [
    "neu_logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(neu_logistic_reg, neu_train_data_features, neu_train_data[output],\n",
    "                        neu_val_data_features, neu_val_data[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5aa51ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61273256\n",
      "Iteration 2, loss = 0.52792750\n",
      "Iteration 3, loss = 0.43478158\n",
      "Iteration 4, loss = 0.33809975\n",
      "Iteration 5, loss = 0.22833281\n",
      "Iteration 6, loss = 0.13797808\n",
      "Iteration 7, loss = 0.07985339\n",
      "Iteration 8, loss = 0.04659375\n",
      "Iteration 9, loss = 0.03015936\n",
      "Iteration 10, loss = 0.02231269\n",
      "Iteration 11, loss = 0.01905966\n",
      "Iteration 12, loss = 0.01628922\n",
      "Iteration 13, loss = 0.01606904\n",
      "Iteration 14, loss = 0.01487527\n",
      "Iteration 15, loss = 0.01411753\n",
      "Iteration 16, loss = 0.01350253\n",
      "Iteration 17, loss = 0.01380622\n",
      "Iteration 18, loss = 0.01318912\n",
      "Iteration 19, loss = 0.01359710\n",
      "Iteration 20, loss = 0.01288595\n",
      "Iteration 21, loss = 0.01268367\n",
      "Iteration 22, loss = 0.01240760\n",
      "Iteration 23, loss = 0.01214126\n",
      "Iteration 24, loss = 0.01244790\n",
      "Iteration 25, loss = 0.01197988\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "score on training data:\n",
      "0.9926769731489016\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.8273615635179153\n",
      "f1_score: \n",
      "0.7399344681531207\n"
     ]
    }
   ],
   "source": [
    "neu_mlp = MLPClassifier(hidden_layer_sizes=(100,100), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(neu_mlp, neu_train_data_features, neu_train_data[output],\n",
    "                        neu_val_data_features, neu_val_data[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52764d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on training data:\n",
      "0.8563873067534581\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.7882736156351792\n",
      "f1_score: \n",
      "0.5174006626520593\n"
     ]
    }
   ],
   "source": [
    "neu_mnb = MultinomialNB()\n",
    "train_n_test_classifier(neu_mnb, neu_train_data_features, neu_train_data[output],\n",
    "                        neu_val_data_features, neu_val_data[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea3a5959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on training data:\n",
      "0.9759967453213995\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.8338762214983714\n",
      "f1_score: \n",
      "0.7070446253157452\n"
     ]
    }
   ],
   "source": [
    "neu_svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(neu_svm, neu_train_data_features, neu_train_data[output],\n",
    "                        neu_val_data_features, neu_val_data[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "458deec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on training data:\n",
      "0.9922701383238405\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score: \n",
      "0.8143322475570033\n",
      "f1_score: \n",
      "0.6622662266226623\n"
     ]
    }
   ],
   "source": [
    "neu_rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(neu_rf, neu_train_data_features, neu_train_data[output],\n",
    "                        neu_val_data_features, neu_val_data[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9b5e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(f'vectorizer.pkl', 'wb'))\n",
    "pickle.dump(logistic_reg, open(f'logistic_reg.pkl', 'wb'))\n",
    "pickle.dump(mnb, open(f'mnb.pkl', 'wb'))\n",
    "pickle.dump(svm, open(f'svm.pkl', 'wb'))\n",
    "pickle.dump(rf, open(f'rf.pkl', 'wb'))\n",
    "pickle.dump(mlp, open(f'mlp.pkl', 'wb'))\n",
    "pickle.dump(neu_vectorizer, open(f'neu_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(neu_logistic_reg, open(f'neu_logistic_reg.pkl', 'wb'))\n",
    "pickle.dump(neu_mnb, open(f'neu_mnb.pkl', 'wb'))\n",
    "pickle.dump(neu_svm, open(f'neu_svm.pkl', 'wb'))\n",
    "pickle.dump(neu_rf, open(f'neu_rf.pkl', 'wb'))\n",
    "pickle.dump(neu_mlp, open(f'neu_mlp.pkl', 'wb'))\n",
    "vectorizer = pickle.load(open(f'vectorizer.pkl', 'rb'))\n",
    "logistic_reg = pickle.load(open(f'logistic_reg.pkl', 'rb'))\n",
    "mnb = pickle.load(open(f'mnb.pkl', 'rb'))\n",
    "svm = pickle.load(open(f'svm.pkl', 'rb'))\n",
    "rf = pickle.load(open(f'rf.pkl', 'rb'))\n",
    "mlp = pickle.load(open(f'mlp.pkl', 'rb'))\n",
    "\n",
    "neu_vectorizer = pickle.load(open(f'neu_vectorizer.pkl', 'rb'))\n",
    "neu_logistic_reg = pickle.load(open(f'neu_logistic_reg.pkl', 'rb'))\n",
    "neu_mnb = pickle.load(open(f'neu_mnb.pkl', 'rb'))\n",
    "neu_svm = pickle.load(open(f'neu_svm.pkl', 'rb'))\n",
    "neu_rf = pickle.load(open(f'neu_rf.pkl', 'rb'))\n",
    "neu_mlp = pickle.load(open(f'neu_mlp.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "caf70823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multi_level(X, neu_vectorizer, neu_clf, vectorizer, clf):\n",
    "    neu_y_pred = neu_clf.predict(neu_vectorizer.transform(X))\n",
    "    if len(X[neu_y_pred == 'غير محايد']) > 0:\n",
    "        y_pred = clf.predict(vectorizer.transform(X[neu_y_pred == 'غير محايد'])) # classify non neutral into positive or negative\n",
    "        neu_y_pred[neu_y_pred == 'غير محايد'] = y_pred\n",
    "    \n",
    "    final_y_pred = neu_y_pred\n",
    "    return final_y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d09ff376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = test_data.dropna()['comment'].values\n",
    "y = test_data.dropna()['label'].values\n",
    "pred_y = predict_multi_level(X, neu_vectorizer, neu_mlp, vectorizer, mnb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46cb3a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: \n",
      "0.8613445378151261\n",
      "f1_score: \n",
      "0.5544733208926919\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score: ')\n",
    "print(accuracy_score(y, pred_y))\n",
    "\n",
    "print('f1_score: ')\n",
    "print(f1_score(y, pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d923362a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"__________________________________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77717c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "      <td>محايد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment   label\n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال    سلبي\n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي   محايد\n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي\n",
       "3                                   رمز الاشتراك شنو   محايد\n",
       "4                                                واو  ايجابي"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('ALL_data.xlsx')\n",
    "data = data.dropna()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b517316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean']=data['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6dbeb145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "      <td>محايد</td>\n",
       "      <td>طيب ما تشرحو طريقه الاشتراك في الباقه دي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "      <td>محايد</td>\n",
       "      <td>رمز الاشتراك شنو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>واو</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment   label  \\\n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال    سلبي   \n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي   محايد   \n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي   \n",
       "3                                   رمز الاشتراك شنو   محايد   \n",
       "4                                                واو  ايجابي   \n",
       "\n",
       "                                               clean  \n",
       "0                   نفسي يوم تكتبو السعر بدون مانسال  \n",
       "1           طيب ما تشرحو طريقه الاشتراك في الباقه دي  \n",
       "2   لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...  \n",
       "3                                   رمز الاشتراك شنو  \n",
       "4                                                واو  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64c86576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "arabicStopWords= stopwords.words(\"arabic\")\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    tokenizedRow = word_tokenize(data['clean'][i])\n",
    "    commentWithNoStopWords= ' '.join([i for i in tokenizedRow if i not in arabicStopWords])\n",
    "                \n",
    "\n",
    "    data['clean'][i]=commentWithNoStopWords\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8287661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'clean']\n",
      "output: label\n",
      "features: ['clean']\n",
      "train data = 2458\n",
      "val  data = 307\n",
      "test  data = 308\n",
      "all data = 3073\n",
      "2458\n",
      "307\n",
      "308\n",
      "3073\n",
      "3073\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.9121236777868186\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.745928338762215\n",
      "f1_score  on test data:\n",
      "0.6897513062744789\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.8986981285598047\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7296416938110749\n",
      "f1_score  on test data:\n",
      "0.6123716011184042\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.9772172497965825\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7817589576547231\n",
      "f1_score  on test data:\n",
      "0.7478114143811098\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.9914564686737185\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7296416938110749\n",
      "f1_score  on test data:\n",
      "0.7051266230650048\n",
      "Iteration 1, loss = 1.26971564\n",
      "Iteration 2, loss = 1.17685498\n",
      "Iteration 3, loss = 1.12718751\n",
      "Iteration 4, loss = 1.08679258\n",
      "Iteration 5, loss = 1.04425171\n",
      "Iteration 6, loss = 0.99018696\n",
      "Iteration 7, loss = 0.91369946\n",
      "Iteration 8, loss = 0.80086047\n",
      "Iteration 9, loss = 0.65771730\n",
      "Iteration 10, loss = 0.49641538\n",
      "Iteration 11, loss = 0.33959634\n",
      "Iteration 12, loss = 0.21294278\n",
      "Iteration 13, loss = 0.13197448\n",
      "Iteration 14, loss = 0.08842854\n",
      "Iteration 15, loss = 0.06582884\n",
      "Iteration 16, loss = 0.05168529\n",
      "Iteration 17, loss = 0.04065457\n",
      "Iteration 18, loss = 0.03447859\n",
      "Iteration 19, loss = 0.03171863\n",
      "Iteration 20, loss = 0.02750135\n",
      "Iteration 21, loss = 0.02541014\n",
      "Iteration 22, loss = 0.02396812\n",
      "Iteration 23, loss = 0.02269914\n",
      "Iteration 24, loss = 0.02146323\n",
      "Iteration 25, loss = 0.02040214\n",
      "Iteration 26, loss = 0.02069164\n",
      "Iteration 27, loss = 0.01958746\n",
      "Iteration 28, loss = 0.01872382\n",
      "Iteration 29, loss = 0.01848265\n",
      "Iteration 30, loss = 0.01928686\n",
      "Iteration 31, loss = 0.01795393\n",
      "Iteration 32, loss = 0.01783916\n",
      "Iteration 33, loss = 0.01809770\n",
      "Iteration 34, loss = 0.01791222\n",
      "Iteration 35, loss = 0.01738779\n",
      "Iteration 36, loss = 0.01744117\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9914564686737185\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7785016286644951\n",
      "f1_score  on test data:\n",
      "0.7566117453784412\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "features.remove('comment')\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "\n",
    "print(\"train data = \"+str(len(train_data)))\n",
    "print(\"val  data = \"+str(len(val_data)))\n",
    "print(\"test  data = \"+str(len(test_data)))\n",
    "\n",
    "print(\"all data = \"+str(len(data)))\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "print(len(train_data)+len(val_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TF IDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['clean'])\n",
    "val_data_features = vectorizer.transform(val_data['clean'])\n",
    "test_data_features = vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, val_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    val_data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    val_data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "# RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a468f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aranorm import normalize_arabic_text\n",
    "for i in range(0,len(data)):\n",
    "\n",
    "    data['clean'][i]=normalize_arabic_text(data['clean'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5ac5a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'clean']\n",
      "output: label\n",
      "features: ['clean']\n",
      "train data = 2458\n",
      "val  data = 307\n",
      "test  data = 308\n",
      "all data = 3073\n",
      "2458\n",
      "307\n",
      "308\n",
      "3073\n",
      "3073\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.9137510170870626\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.752442996742671\n",
      "f1_score  on test data:\n",
      "0.6968363006023971\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.8958502847843776\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7231270358306189\n",
      "f1_score  on test data:\n",
      "0.6055293884106618\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.9772172497965825\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.762214983713355\n",
      "f1_score  on test data:\n",
      "0.723831417624521\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.9902359641985354\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.739413680781759\n",
      "f1_score  on test data:\n",
      "0.7194069238666438\n",
      "Iteration 1, loss = 1.09043317\n",
      "Iteration 2, loss = 1.05550792\n",
      "Iteration 3, loss = 1.02865963\n",
      "Iteration 4, loss = 0.99732348\n",
      "Iteration 5, loss = 0.95404671\n",
      "Iteration 6, loss = 0.89142760\n",
      "Iteration 7, loss = 0.79368700\n",
      "Iteration 8, loss = 0.64616555\n",
      "Iteration 9, loss = 0.44517813\n",
      "Iteration 10, loss = 0.24584185\n",
      "Iteration 11, loss = 0.11662699\n",
      "Iteration 12, loss = 0.06256918\n",
      "Iteration 13, loss = 0.04154463\n",
      "Iteration 14, loss = 0.03272393\n",
      "Iteration 15, loss = 0.02795912\n",
      "Iteration 16, loss = 0.02514689\n",
      "Iteration 17, loss = 0.02394640\n",
      "Iteration 18, loss = 0.02222615\n",
      "Iteration 19, loss = 0.02196878\n",
      "Iteration 20, loss = 0.02101716\n",
      "Iteration 21, loss = 0.02052626\n",
      "Iteration 22, loss = 0.02023659\n",
      "Iteration 23, loss = 0.02041094\n",
      "Iteration 24, loss = 0.02048043\n",
      "Iteration 25, loss = 0.01903897\n",
      "Iteration 26, loss = 0.01953052\n",
      "Iteration 27, loss = 0.01953335\n",
      "Iteration 28, loss = 0.01923608\n",
      "Iteration 29, loss = 0.01951264\n",
      "Iteration 30, loss = 0.01933626\n",
      "Iteration 31, loss = 0.01855626\n",
      "Iteration 32, loss = 0.01842794\n",
      "Iteration 33, loss = 0.01855253\n",
      "Iteration 34, loss = 0.01775153\n",
      "Iteration 35, loss = 0.01821967\n",
      "Iteration 36, loss = 0.01801163\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9902359641985354\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7882736156351792\n",
      "f1_score  on test data:\n",
      "0.7656866260134021\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "features.remove('comment')\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "\n",
    "print(\"train data = \"+str(len(train_data)))\n",
    "print(\"val  data = \"+str(len(val_data)))\n",
    "print(\"test  data = \"+str(len(test_data)))\n",
    "\n",
    "print(\"all data = \"+str(len(data)))\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "print(len(train_data)+len(val_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TF IDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['clean'])\n",
    "val_data_features = vectorizer.transform(val_data['clean'])\n",
    "test_data_features = vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, val_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    val_data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    val_data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "# RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e00c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 20:19:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b7116a303f4ee9806bb7ad15a9d186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 20:19:59 WARNING: Language ar package default expects mwt, which has been added\n",
      "2023-01-02 20:19:59 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| lemma     | padt    |\n",
      "=======================\n",
      "\n",
      "2023-01-02 20:19:59 INFO: Use device: cpu\n",
      "2023-01-02 20:19:59 INFO: Loading: tokenize\n",
      "2023-01-02 20:19:59 INFO: Loading: mwt\n",
      "2023-01-02 20:19:59 INFO: Loading: lemma\n",
      "2023-01-02 20:19:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline(lang='ar', processors='tokenize,lemma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "131f77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,len(data)):\n",
    "    doc = nlp(data['clean'][i])\n",
    "    lema=''.join(word.lemma+' ' for sent in doc.sentences for word in sent.words)\n",
    "\n",
    "    data['clean'][i]=lema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d361c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'clean']\n",
      "output: label\n",
      "features: ['clean']\n",
      "train data = 2458\n",
      "val  data = 307\n",
      "test  data = 308\n",
      "all data = 3073\n",
      "2458\n",
      "307\n",
      "308\n",
      "3073\n",
      "3073\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.8315703824247356\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7068403908794788\n",
      "f1_score  on test data:\n",
      "0.6375790018174123\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.7424735557363711\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.6677524429967426\n",
      "f1_score  on test data:\n",
      "0.523066200324412\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.8759153783563873\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7231270358306189\n",
      "f1_score  on test data:\n",
      "0.6629254290574128\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.9499593165174939\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7035830618892508\n",
      "f1_score  on test data:\n",
      "0.6506224602167882\n",
      "Iteration 1, loss = 1.16489309\n",
      "Iteration 2, loss = 1.07858581\n",
      "Iteration 3, loss = 1.02133771\n",
      "Iteration 4, loss = 0.98000513\n",
      "Iteration 5, loss = 0.93616728\n",
      "Iteration 6, loss = 0.87068314\n",
      "Iteration 7, loss = 0.77595460\n",
      "Iteration 8, loss = 0.66526791\n",
      "Iteration 9, loss = 0.55533686\n",
      "Iteration 10, loss = 0.45286870\n",
      "Iteration 11, loss = 0.35972553\n",
      "Iteration 12, loss = 0.28405560\n",
      "Iteration 13, loss = 0.23200560\n",
      "Iteration 14, loss = 0.19438219\n",
      "Iteration 15, loss = 0.17096913\n",
      "Iteration 16, loss = 0.15698258\n",
      "Iteration 17, loss = 0.14757370\n",
      "Iteration 18, loss = 0.14068637\n",
      "Iteration 19, loss = 0.13371742\n",
      "Iteration 20, loss = 0.12724642\n",
      "Iteration 21, loss = 0.12393611\n",
      "Iteration 22, loss = 0.12104530\n",
      "Iteration 23, loss = 0.11900758\n",
      "Iteration 24, loss = 0.11774626\n",
      "Iteration 25, loss = 0.11584820\n",
      "Iteration 26, loss = 0.11648055\n",
      "Iteration 27, loss = 0.11393594\n",
      "Iteration 28, loss = 0.11422095\n",
      "Iteration 29, loss = 0.11352536\n",
      "Iteration 30, loss = 0.11203825\n",
      "Iteration 31, loss = 0.11066112\n",
      "Iteration 32, loss = 0.11218638\n",
      "Iteration 33, loss = 0.11231365\n",
      "Iteration 34, loss = 0.10958525\n",
      "Iteration 35, loss = 0.10908097\n",
      "Iteration 36, loss = 0.10851391\n",
      "Iteration 37, loss = 0.10854517\n",
      "Iteration 38, loss = 0.10856996\n",
      "Iteration 39, loss = 0.11002245\n",
      "Iteration 40, loss = 0.10762034\n",
      "Iteration 41, loss = 0.10781812\n",
      "Iteration 42, loss = 0.10744147\n",
      "Iteration 43, loss = 0.10800377\n",
      "Iteration 44, loss = 0.10895611\n",
      "Iteration 45, loss = 0.10864129\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9487388120423108\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.6872964169381107\n",
      "f1_score  on test data:\n",
      "0.642460824956701\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "features.remove('comment')\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "\n",
    "print(\"train data = \"+str(len(train_data)))\n",
    "print(\"val  data = \"+str(len(val_data)))\n",
    "print(\"test  data = \"+str(len(test_data)))\n",
    "\n",
    "print(\"all data = \"+str(len(data)))\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "print(len(train_data)+len(val_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TF IDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['clean'])\n",
    "val_data_features = vectorizer.transform(val_data['clean'])\n",
    "test_data_features = vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, val_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    val_data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    val_data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "# RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e02b21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,len(data)):  \n",
    "    data['clean'][i]=normalize_arabic_text(data['clean'][i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e81b2380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: label\n",
      "features: ['comment', 'clean']\n",
      "output: label\n",
      "features: ['clean']\n",
      "train data = 2458\n",
      "val  data = 307\n",
      "test  data = 308\n",
      "all data = 3073\n",
      "2458\n",
      "307\n",
      "308\n",
      "3073\n",
      "3073\n",
      "----------------------------------------------------------------------------------------------------LogisticRegress\n",
      "accuracy_score Score on training data:\n",
      "0.9202603742880391\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.758957654723127\n",
      "f1_score  on test data:\n",
      "0.7015155479546422\n",
      "----------------------------------------------------------------------------------------------------MultinomialNB()\n",
      "accuracy_score Score on training data:\n",
      "0.8641171684296176\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7296416938110749\n",
      "f1_score  on test data:\n",
      "0.6154767325380203\n",
      "----------------------------------------------------------------------------------------------------SVC(kernel='lin\n",
      "accuracy_score Score on training data:\n",
      "0.9703010577705452\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.7785016286644951\n",
      "f1_score  on test data:\n",
      "0.7381975105036299\n",
      "----------------------------------------------------------------------------------------------------RandomForestCla\n",
      "accuracy_score Score on training data:\n",
      "0.9894222945484134\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.742671009771987\n",
      "f1_score  on test data:\n",
      "0.7095633059260904\n",
      "Iteration 1, loss = 1.04466199\n",
      "Iteration 2, loss = 1.01828181\n",
      "Iteration 3, loss = 0.98089249\n",
      "Iteration 4, loss = 0.91640689\n",
      "Iteration 5, loss = 0.82730399\n",
      "Iteration 6, loss = 0.71287437\n",
      "Iteration 7, loss = 0.58290943\n",
      "Iteration 8, loss = 0.47045638\n",
      "Iteration 9, loss = 0.37954316\n",
      "Iteration 10, loss = 0.29801554\n",
      "Iteration 11, loss = 0.22487338\n",
      "Iteration 12, loss = 0.15896686\n",
      "Iteration 13, loss = 0.10733301\n",
      "Iteration 14, loss = 0.07296903\n",
      "Iteration 15, loss = 0.05335375\n",
      "Iteration 16, loss = 0.04196032\n",
      "Iteration 17, loss = 0.03562776\n",
      "Iteration 18, loss = 0.03178110\n",
      "Iteration 19, loss = 0.02880052\n",
      "Iteration 20, loss = 0.02727980\n",
      "Iteration 21, loss = 0.02538865\n",
      "Iteration 22, loss = 0.02414052\n",
      "Iteration 23, loss = 0.02287718\n",
      "Iteration 24, loss = 0.02337967\n",
      "Iteration 25, loss = 0.02251203\n",
      "Iteration 26, loss = 0.02268795\n",
      "Iteration 27, loss = 0.02254885\n",
      "Iteration 28, loss = 0.02243943\n",
      "Iteration 29, loss = 0.02302233\n",
      "Iteration 30, loss = 0.02181703\n",
      "Iteration 31, loss = 0.02144788\n",
      "Iteration 32, loss = 0.02096966\n",
      "Iteration 33, loss = 0.02101718\n",
      "Iteration 34, loss = 0.02172983\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "----------------------------------------------------------------------------------------------------MLPClassifier(h\n",
      "accuracy_score Score on training data:\n",
      "0.9894222945484134\n",
      "____________________________________________________________________________________________________\n",
      "score on testing data:\n",
      "accuracy_score Score on test data:\n",
      "0.8110749185667753\n",
      "f1_score  on test data:\n",
      "0.790920245398773\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT\n",
    "\n",
    "def random_split(data, features, output, fraction, seed=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                        data[output],\n",
    "                                                        stratify = data[output],\n",
    "                                                        random_state=seed,\n",
    "                                                        train_size=fraction\n",
    "                                                       )\n",
    "    train_data = pd.DataFrame(data=X_train, columns=features)\n",
    "    train_data[output] = y_train\n",
    "    test_data = pd.DataFrame(data=X_test, columns=features)\n",
    "    test_data[output] = y_test\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "#LABE4L FEATURE\n",
    "\n",
    "\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "train_fraction = .80 # use this to split data into training (80%), and tmp (20%)\n",
    "val_fraction = .50   # use this to split the tmp data into validation (50%), and \n",
    "                     # testing (50%) which means that the validation will be 10% of the original data as well as the\n",
    "\n",
    "output = 'label' # output label column\n",
    "features = data.columns.tolist() # the features columns\n",
    "features.remove(output)\n",
    "features.remove('comment')\n",
    "print('output:', output)\n",
    "print('features:', features)\n",
    "\n",
    "train_data, tmp = random_split(data, features, output, train_fraction, rand_seed)\n",
    "val_data, test_data = random_split(tmp, features, output, val_fraction, rand_seed)\n",
    "\n",
    "print(\"train data = \"+str(len(train_data)))\n",
    "print(\"val  data = \"+str(len(val_data)))\n",
    "print(\"test  data = \"+str(len(test_data)))\n",
    "\n",
    "print(\"all data = \"+str(len(data)))\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "print(len(train_data)+len(val_data)+len(test_data))\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "# TF IDF\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, max_df=0.5, stop_words=None, use_idf=True)\n",
    "train_data_features = vectorizer.fit_transform(train_data['clean'])\n",
    "val_data_features = vectorizer.transform(val_data['clean'])\n",
    "test_data_features = vectorizer.transform(test_data['clean'])\n",
    "\n",
    "#SHAPE\n",
    "\n",
    "train_data_features.shape, val_data_features.shape, test_data_features.shape\n",
    "\n",
    "#FUNCTION FOR MODEL TRAIN\n",
    "\n",
    "def train_n_test_classifier(clf, train_features, train_labels, test_features, test_labels,data):\n",
    "    clf.fit(train_features, train_labels) # please learn patterns from the data\n",
    "\n",
    "   \n",
    "    print('-'*100+str(clf)[0:15])\n",
    "    print(\"accuracy_score Score on training data:\")\n",
    "    print(clf.score(train_features, train_labels))\n",
    "    \n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "    print(\"score on testing data:\")\n",
    "    \n",
    "    pred_y = clf.predict(test_features)\n",
    "    \n",
    "    val_data['predict']=pred_y\n",
    " \n",
    "    count=0\n",
    "    \n",
    "    print(\"accuracy_score Score on test data:\")\n",
    "    print(accuracy_score(test_labels, pred_y))\n",
    "    \n",
    "    print(\"f1_score  on test data:\")\n",
    "    print(f1_score(test_labels, pred_y, average='macro'))\n",
    "    filename='result_'+str(clf)[0:15]+'.xlsx'\n",
    "    val_data.to_excel(filename)\n",
    "    \n",
    "    \n",
    "\n",
    "#LOGASTIC REGRESSION\n",
    "\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(logistic_reg, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "\n",
    "# MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "train_n_test_classifier(mnb, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# S V M\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(svm, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "\n",
    "# RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rand_seed)\n",
    "\n",
    "train_n_test_classifier(rf, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n",
    "\n",
    "# MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20), verbose=True, tol=0.001, random_state=rand_seed)\n",
    "train_n_test_classifier(mlp, train_data_features, train_data[output],\n",
    "                        val_data_features, val_data[output],val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fbeb829e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>واو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>سوداني جميل</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>سوداني جميل</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment   label  \\\n",
       "0                    نفسي يوم تكتبو السعر بدون مانسال    سلبي   \n",
       "2    لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي   \n",
       "4                                                 واو  ايجابي   \n",
       "8   شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...  ايجابي   \n",
       "13                                        سوداني جميل  ايجابي   \n",
       "\n",
       "                                                clean  \n",
       "0                    نفسي يوم تكتبو السعر بدون مانسال  \n",
       "2    لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...  \n",
       "4                                                 واو  \n",
       "8   شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...  \n",
       "13                                        سوداني جميل  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = pd.read_excel('ALL_data.xlsx')\n",
    "Data = data.dropna()\n",
    "Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "854b8548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نفسي يوم تكتبو السعر بدون مانسال\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>نفسي يوم تكتبو السعر بدون مانسال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "      <td>سلبي</td>\n",
       "      <td>لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>واو</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>واو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>سوداني جميل</td>\n",
       "      <td>ايجابي</td>\n",
       "      <td>سوداني جميل</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment   label  \\\n",
       "0                    نفسي يوم تكتبو السعر بدون مانسال    سلبي   \n",
       "2    لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...    سلبي   \n",
       "4                                                 واو  ايجابي   \n",
       "8   شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...  ايجابي   \n",
       "13                                        سوداني جميل  ايجابي   \n",
       "\n",
       "                                                clean  \n",
       "0                    نفسي يوم تكتبو السعر بدون مانسال  \n",
       "2    لو قللتو الرسائل دي واديتونا ليها ميقات يكون ...  \n",
       "4                                                 واو  \n",
       "8   شكرا التوضيح مفيد اكرر الشكر سوداني الابداع وا...  \n",
       "13                                        سوداني جميل  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = Data[Data['label'] != 'محايد']\n",
    "Data['clean']=Data['comment']\n",
    "print(Data['clean'][0])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ccd5f421",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [105]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m arabicStopWords\u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marabic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(Data)):\n\u001b[1;32m----> 7\u001b[0m     tokenizedRow \u001b[38;5;241m=\u001b[39m word_tokenize(\u001b[43mData\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      8\u001b[0m     commentWithNoStopWords\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tokenizedRow \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m arabicStopWords])\n\u001b[0;32m     11\u001b[0m     Data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m=\u001b[39mcommentWithNoStopWords\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "arabicStopWords= stopwords.words(\"arabic\")\n",
    "\n",
    "for i in Data['clean']:\n",
    "    tokenizedRow = word_tokenize(i)\n",
    "    commentWithNoStopWords= ' '.join([i for i in tokenizedRow if i not in arabicStopWords])\n",
    "                \n",
    "\n",
    "    Data['clean'][i]=commentWithNoStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ead39341",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m arabicStopWords\u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marabic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m----> 7\u001b[0m     tokenizedRow \u001b[38;5;241m=\u001b[39m word_tokenize(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      8\u001b[0m     commentWithNoStopWords\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tokenizedRow \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m arabicStopWords])\n\u001b[0;32m     11\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m=\u001b[39mcommentWithNoStopWords\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5cd359",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data)):\n",
    "    tokenizedRow = word_tokenize(data['clean'][i])\n",
    "    commentWithNoStopWords= ' '.join([i for i in tokenizedRow if i not in arabicStopWords])\n",
    "                \n",
    "\n",
    "    data['clean'][i]=commentWithNoStopWords\n",
    "    \n",
    "\n",
    "for i in range(0,len(data)):\n",
    "\n",
    "    data['clean'][i]=normalize_arabic_text(data['clean'][i])\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    doc = nlp(data['clean'][i])\n",
    "    lema=''.join(word.lemma+' ' for sent in doc.sentences for word in sent.words)\n",
    "\n",
    "    data['clean'][i]=lema\n",
    "\n",
    "for i in range(0,len(data)):  \n",
    "    data['clean'][i]=normalize_arabic_text(data['clean'][i])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
